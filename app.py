import io
import time
import logging
from datetime import datetime, timedelta

import pandas as pd
import requests
import streamlit as st
import openpyxl

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
log_file = f"twitter_crawler_{datetime.now().strftime('%Y%m%d')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")

from twitter_crawler.data_prep import prepare_from_excel, normalize_company_name, read_first_column_as_list
from twitter_crawler.twitter_api import TwitterAPI
from twitter_crawler.accounts import search_account_for_company
from twitter_crawler.sentiment import sentiment_score
from twitter_crawler.storage import AccountInfo, save_accounts_csv, load_accounts_csv
from twitter_crawler.utils_date import (
    today_range, this_week_range, this_month_range, this_quarter_range,
    this_half_year_range, this_year_range, recent_days_range
)
from twitter_crawler.config import get_settings, Settings


st.set_page_config(page_title="æ¨ç‰¹æŠ“å–ä¸åˆ†æå·¥å…·", page_icon="ğŸ§©", layout="wide")
st.title("ğŸ§© æ¨ç‰¹æŠ“å–ä¸åˆ†æå·¥å…·ï¼ˆStreamlitï¼‰")
st.caption("æ”¯æŒï¼šæ•°æ®å‡†å¤‡ã€è´¦å·æŸ¥æ‰¾ã€æ¨æ–‡æŠ“å–ä¸æƒ…ç»ªã€æ•°é‡æŸ¥è¯¢ã€æ‰¹é‡ä»»åŠ¡")

# ä¾§è¾¹æ ï¼šAPI ä¸é€Ÿç‡é™åˆ¶è®¾ç½®ï¼ˆæ˜¾ç¤ºé»˜è®¤å€¼å¹¶å…è®¸ä¿®æ”¹ï¼‰
st.sidebar.header("API ä¸é€Ÿç‡é™åˆ¶è®¾ç½®")
default_settings = get_settings()
if "api_settings" not in st.session_state:
    st.session_state.api_settings = default_settings

with st.sidebar.expander("æŸ¥çœ‹/ä¿®æ”¹è®¾ç½®", expanded=True):
    token = st.text_input("Bearer Tokenï¼ˆç•™ç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰", value="", type="password")
    use_all = st.checkbox("ä½¿ç”¨å…¨é‡å†å² /tweets/search/allï¼ˆéœ€ Academicï¼‰", value=st.session_state.api_settings.use_search_all)
    rpm = st.number_input("æ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼ˆRequests Per Minuteï¼‰", min_value=1, max_value=300, value=st.session_state.api_settings.requests_per_minute, step=1)
    max_retries = st.number_input("æœ€å¤§é‡è¯•æ¬¡æ•°", min_value=0, max_value=20, value=st.session_state.api_settings.rate_limit_max_retries, step=1)
    base_delay = st.number_input("åŸºç¡€é€€é¿ç§’æ•°", min_value=0.0, max_value=120.0, value=float(st.session_state.api_settings.rate_limit_base_delay_seconds), step=0.5)
    max_delay = st.number_input("æœ€å¤§é€€é¿ç§’æ•°", min_value=0.0, max_value=600.0, value=float(st.session_state.api_settings.rate_limit_max_delay_seconds), step=1.0)

    # å®æ—¶æ›´æ–°è®¾ç½®ï¼Œæ— éœ€åº”ç”¨æŒ‰é’®
    st.session_state.api_settings = Settings(
        twitter_bearer_token=token or default_settings.twitter_bearer_token,
        use_search_all=use_all,
        rate_limit_max_retries=int(max_retries),
        rate_limit_base_delay_seconds=float(base_delay),
        rate_limit_max_delay_seconds=float(max_delay),
        requests_per_minute=int(rpm),
    )

# æ„é€  API å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨å½“å‰è®¾ç½®ï¼‰
api = TwitterAPI(
    bearer_token=st.session_state.api_settings.twitter_bearer_token,
    settings=st.session_state.api_settings
)

tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([
    "1ï¼šæ•°æ®å‡†å¤‡ä¸æ¸…æ´—",
    "2ï¼šæ¨ç‰¹è´¦å·æŸ¥æ‰¾ä¸ç®¡ç†",
    "3ï¼šæ¨æ–‡æŠ“å–ä¸æƒ…ç»ªåˆ†æ",
    "4ï¼šæ¨æ–‡æ•°é‡æŸ¥è¯¢",
    "5ï¼šæ‰¹é‡è´¦å·æŸ¥æ‰¾ä¸ç®¡ç†",
    "6ï¼šæ‰¹é‡æ¨æ–‡æ•°é‡æŸ¥è¯¢",
    "7ï¼šæ‰¹é‡æ¨æ–‡å†…å®¹æŸ¥è¯¢",
    "8ï¼šæ‰¹é‡æŸ¥è¯¢æ¨æ–‡"
])

with tab1:
    st.subheader("1ï¼šæ•°æ®å‡†å¤‡ä¸æ¸…æ´—")
    excel_file = st.file_uploader("ä¸Šä¼ ã€æ¨ç‰¹å…¬å¸æ ·æœ¬.xlsxã€", type=["xlsx"])
    col_a, col_b = st.columns(2)
    with col_a:
        run_prep = st.button("å¼€å§‹æ¸…æ´—å¹¶ç”Ÿæˆæ˜ å°„")
    if run_prep and excel_file:
        with st.spinner("å¤„ç†ä¸­..."):
            buf = io.BytesIO(excel_file.read())
            # å°†ä¸Šä¼ æ–‡ä»¶å†™åˆ°ä¸´æ—¶ DataFrame å¤„ç†
            df = pd.read_excel(buf, engine="openpyxl")
            temp_path = "ä¸´æ—¶_æ¨ç‰¹å…¬å¸æ ·æœ¬.xlsx"
            df.to_excel(temp_path, index=False)
            unique_names, mapping = prepare_from_excel(temp_path, "normalized_companies.csv", "name_mapping.csv")
        st.success(f"å®Œæˆï¼æ ‡å‡†åŒ–å…¬å¸æ•°ï¼š{len(unique_names)}")
        st.download_button("ä¸‹è½½ æ ‡å‡†åŒ–å…¬å¸åˆ—è¡¨ CSV", data=open("normalized_companies.csv", "rb").read(), file_name="normalized_companies.csv")
        st.download_button("ä¸‹è½½ åç§°æ˜ å°„ CSV", data=open("name_mapping.csv", "rb").read(), file_name="name_mapping.csv")

with tab2:
    st.subheader("2ï¼šæ¨ç‰¹è´¦å·æŸ¥æ‰¾ä¸ç®¡ç†")
    st.caption("ä¼˜å…ˆ verifiedï¼›å¤šç»“æœå¯åœ¨ CLI ä¸­ç¡®è®¤ã€‚æ­¤å¤„æä¾›åŸºç¡€å¿«é€ŸåŒ¹é…ã€‚")
    names_input = st.text_area("è¾“å…¥å…¬å¸åç§°ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    col1, col2 = st.columns(2)
    with col1:
        run_find = st.button("æŸ¥æ‰¾è´¦å·")
    with col2:
        uploaded_map = st.file_uploader("å¯¼å…¥ç°æœ‰è´¦å·æ˜ å°„ CSVï¼ˆå¯é€‰ï¼‰", type=["csv"])
    results_df = None
    if run_find and names_input.strip():
        names = [normalize_company_name(x) for x in names_input.splitlines() if x.strip()]
        results: list[AccountInfo] = []
        try:
            with st.spinner("æ­£åœ¨æŸ¥æ‰¾..."):
                for comp in names:
                    try:
                        acc = search_account_for_company(api, comp)
                        if acc:
                            results.append(acc)
                    except requests.exceptions.HTTPError as e:
                        if "401" in str(e):
                            st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                            break
                        elif "429" in str(e):
                            st.warning(f"æŸ¥æ‰¾{comp}æ—¶é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                            time.sleep(5)  # æš‚åœ5ç§’å†ç»§ç»­
                        else:
                            st.warning(f"æŸ¥æ‰¾{comp}æ—¶å‡ºé”™ï¼š{str(e)}")
                            continue
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()
        if results:
            results_df = pd.DataFrame([a.__dict__ for a in results])
            st.dataframe(results_df, use_container_width=True)
            save_accounts_csv(results, "company_account_map.csv")
            st.download_button("ä¸‹è½½è´¦å·æ˜ å°„ CSV", data=open("company_account_map.csv", "rb").read(), file_name="company_account_map.csv")
        else:
            st.info("æœªæ‰¾åˆ°ä»»ä½•è´¦å·ï¼Œè¯·å°è¯•ä¸åŒåç§°ã€‚")
    if uploaded_map is not None:
        df_map = pd.read_csv(uploaded_map)
        st.dataframe(df_map, use_container_width=True)

with tab3:
    st.subheader("3ï¼šæ¨æ–‡æŠ“å–ä¸æƒ…ç»ªåˆ†æ")
    # ç°åœ¨è®¾ç½®ä¼šå®æ—¶æ›´æ–°ï¼Œç›´æ¥ä½¿ç”¨session_stateä¸­çš„å€¼
    st.info(f"å½“å‰é…ç½®ï¼š{'Academic æƒé™ï¼ˆæ”¯æŒå…¨é‡å†å²ï¼‰' if st.session_state.api_settings.use_search_all else 'Basic Planï¼ˆä»…æ”¯æŒæœ€è¿‘7å¤©ï¼‰'}")

    by = st.selectbox("æŸ¥è¯¢æ–¹å¼", ["æŒ‰è´¦å·", "æŒ‰æ¨ç‰¹è´¦å·ç›´æ¥æŸ¥è¯¢", "æŒ‰å…³é”®å­—"])
    value = st.text_input("è´¦å·ï¼ˆä¸å«@ï¼‰æˆ–å…³é”®å­—")

    # æ ¹æ®æƒé™è®¾ç½®æ—¥æœŸèŒƒå›´é»˜è®¤å€¼å’Œé™åˆ¶
    if st.session_state.api_settings.use_search_all:
        default_start = datetime(2006, 1, 1).date()
        default_end = datetime(2022, 12, 31).date()
        st.caption("æ³¨æ„ï¼šAcademic æƒé™æ”¯æŒ 2006-2022 å…¨é‡å†å²æ•°æ®")
    else:
        default_end = datetime.now().date()
        default_start = default_end - timedelta(days=7)
        st.caption("æ³¨æ„ï¼šBasic Plan ä»…æ”¯æŒæœ€è¿‘ 7 å¤©çš„æ¨æ–‡æ•°æ®")

    col1, col2, col3 = st.columns(3)
    with col1:
        start_date = st.date_input("å¼€å§‹æ—¥æœŸ", default_start)
    with col2:
        end_date = st.date_input("ç»“æŸæ—¥æœŸ", default_end)
    # ç§»é™¤åŒ…å«è½¬æ¨åŠŸèƒ½
    run_fetch = st.button("æŠ“å–å¹¶åˆ†æ")

    if run_fetch and value.strip():
        # æ£€æŸ¥Basic Planä¸‹çš„æ—¶é—´èŒƒå›´é™åˆ¶
        if not st.session_state.api_settings.use_search_all:
            # è·å–å½“å‰æ—¶é—´
            current_datetime = datetime.now()
            current_date = current_datetime.date()

            # 1. ç¡®ä¿end_timeä¸è¶…è¿‡å½“å‰æ—¶é—´
            if end_date > current_date:
                st.warning(f"ç»“æŸæ—¥æœŸä¸èƒ½è¶…è¿‡å½“å‰æ—¥æœŸï¼Œå·²è°ƒæ•´ä¸ºä»Šå¤©")
                end_date = current_date
                # å¦‚æœæ˜¯ä»Šå¤©ï¼Œend_timeåº”è¯¥æ˜¯å½“å‰æ—¶é—´è€Œä¸æ˜¯23:59:59
                end_time = current_datetime
            else:
                # éä»Šå¤©ï¼Œä½¿ç”¨å½“å¤©çš„23:59:59
                end_time = datetime.combine(end_date, datetime.max.time())

            # 2. è®¡ç®—æœ€æ—©å…è®¸çš„å¼€å§‹æ—¥æœŸï¼ˆå½“å‰æ—¶é—´-7å¤©ï¼‰
            min_start_date = current_date - timedelta(days=7)

            # ç¡®ä¿start_timeä¸æ—©äºå½“å‰æ—¶é—´-7å¤©
            if start_date < min_start_date:
                st.warning(f"å¼€å§‹æ—¥æœŸä¸èƒ½æ—©äº7å¤©å‰ï¼Œå·²è°ƒæ•´ä¸ºæœ€æ—©å…è®¸æ—¥æœŸ")
                start_date = min_start_date

            # 3. ç¡®ä¿æ—¥æœŸè·¨åº¦ä¸è¶…è¿‡7å¤©
            date_diff = (end_date - start_date).days
            if date_diff > 7:
                st.warning(f"Basic Plan ä»…æ”¯æŒ7å¤©å†…çš„æ•°æ®ï¼Œå·²è‡ªåŠ¨è°ƒæ•´å¼€å§‹æ—¥æœŸ")
                start_date = end_date - timedelta(days=7)

        # åœ¨å¯èƒ½è°ƒæ•´æ—¥æœŸåï¼Œé‡æ–°è®¡ç®—ISOæ ¼å¼æ—¥æœŸ
        start_iso = f"{start_date.strftime('%Y-%m-%d')}T00:00:00Z"
        # å¦‚æœæ˜¯å½“å¤©ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ï¼Œå¦åˆ™ä½¿ç”¨23:59:59
        if 'end_time' in locals() and end_date == current_date:
            end_iso = end_time.strftime('%Y-%m-%dT%H:%M:%SZ')
        else:
            end_iso = f"{end_date.strftime('%Y-%m-%d')}T23:59:59Z"
        all_rows = []

        try:
            with st.spinner("æŠ“å–æ¨æ–‡..."):
                # æ ¹æ®æƒé™å’ŒæŸ¥è¯¢æ–¹å¼é€‰æ‹©ä¸åŒçš„APIè°ƒç”¨æ–¹æ³•
                if by == "æŒ‰è´¦å·" and not st.session_state.api_settings.use_search_all:
                    # Basic Planï¼šä½¿ç”¨ get_user_tweets ç«¯ç‚¹
                    # å…ˆè·å–ç”¨æˆ·ID
                    user_info = api.get_user_by_username(value)
                    if not user_info:
                        st.error(f"æœªæ‰¾åˆ°è´¦å·: {value}")
                        st.stop()

                    user_id = user_info.get("id")
                    # ä½¿ç”¨ next_token è¿›è¡Œåˆ†é¡µ
                    has_next = True
                    next_token = None

                    while has_next:
                        try:
                            resp = api.get_user_tweets(
                                user_id=user_id,
                                start_time=start_iso,
                                end_time=end_iso,
                                exclude="replies,retweets",
                                pagination_token=next_token,
                                max_results=100
                            )
                            data = resp.get("data", [])
                            for t in data:
                                text = t.get("text", "").replace("\n", " ")
                                all_rows.append({
                                    "å…¬å¸åç§°": value,
                                    "æ¨æ–‡å†…å®¹": text,
                                    "å‘å¸ƒæ—¶é—´": t.get("created_at"),
                                    "æƒ…ç»ªåˆ†æ•°": sentiment_score(text),
                                })

                            # ä½¿ç”¨ next_token è¿›è¡Œåˆ†é¡µ
                            next_token = resp.get("meta", {}).get("next_token")
                            has_next = bool(next_token)
                        except requests.exceptions.HTTPError as e:
                            if "401" in str(e):
                                st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                                has_next = False
                                break
                            elif "429" in str(e):
                                st.warning("é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                                time.sleep(10)
                                continue
                            else:
                                st.error(f"æŠ“å–æ¨æ–‡æ—¶å‡ºé”™ï¼š{str(e)}")
                                has_next = False
                elif by == "æŒ‰æ¨ç‰¹è´¦å·ç›´æ¥æŸ¥è¯¢":
                    # ç›´æ¥ä½¿ç”¨search_tweetsç«¯ç‚¹æŸ¥è¯¢è´¦å·ï¼Œä¸éœ€è¦è·å–ç”¨æˆ·ID
                    query = f"from:{value} -is:retweet"
                    next_token = None

                    while True:
                        try:
                            resp = api.search_tweets(
                                query=query,
                                start_time=start_iso,
                                end_time=end_iso,
                                max_results=100,
                                next_token=next_token
                            )
                            data = resp.get("data", [])
                            for t in data:
                                text = t.get("text", "").replace("\n", " ")
                                all_rows.append({
                                    "å…¬å¸åç§°": value,
                                    "æ¨æ–‡å†…å®¹": text,
                                    "å‘å¸ƒæ—¶é—´": t.get("created_at"),
                                    "æƒ…ç»ªåˆ†æ•°": sentiment_score(text),
                                })
                            next_token = resp.get("meta", {}).get("next_token")
                            if not next_token:
                                break
                        except requests.exceptions.HTTPError as e:
                            if "401" in str(e):
                                st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                                break
                            elif "429" in str(e):
                                st.warning("é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                                time.sleep(10)
                            else:
                                st.error(f"æŠ“å–æ¨æ–‡æ—¶å‡ºé”™ï¼š{str(e)}")
                                break
                        except Exception as e:
                            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
                            break
                else:
                    # Academic æƒé™ã€æŒ‰å…³é”®å­—æŸ¥è¯¢æˆ–æŒ‰æ¨ç‰¹è´¦å·ç›´æ¥æŸ¥è¯¢ï¼šä½¿ç”¨ search_tweets ç«¯ç‚¹
                    query = f"from:{value}" if by in ["æŒ‰è´¦å·", "æŒ‰æ¨ç‰¹è´¦å·ç›´æ¥æŸ¥è¯¢"] else value
                    # é»˜è®¤æ’é™¤è½¬æ¨
                    query += " -is:retweet"

                    next_token = None
                    while True:
                        try:
                            resp = api.search_tweets(
                                query=query,
                                start_time=start_iso,
                                end_time=end_iso,
                                expansions=["author_id"],
                                max_results=100,
                                next_token=next_token
                            )
                            data = resp.get("data", [])
                            for t in data:
                                text = t.get("text", "").replace("\n", " ")
                                all_rows.append({
                                    "å…¬å¸åç§°": value if by == "æŒ‰è´¦å·" else "",
                                    "æ¨æ–‡å†…å®¹": text,
                                    "å‘å¸ƒæ—¶é—´": t.get("created_at"),
                                    "æƒ…ç»ªåˆ†æ•°": sentiment_score(text),
                                })
                            next_token = resp.get("meta", {}).get("next_token")
                            if not next_token:
                                break
                        except requests.exceptions.HTTPError as e:
                            if "401" in str(e):
                                st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                                break
                            elif "429" in str(e):
                                st.warning("é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                                time.sleep(10)
                            else:
                                st.error(f"æŠ“å–æ¨æ–‡æ—¶å‡ºé”™ï¼š{str(e)}")
                                break
                        except Exception as e:
                            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
                            break
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()

        if all_rows:
            out_df = pd.DataFrame(all_rows)
            st.dataframe(out_df, use_container_width=True)

            # è®¡ç®—å¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            avg_sentiment = out_df["æƒ…ç»ªåˆ†æ•°"].mean()
            st.metric("å¹³å‡æƒ…ç»ªåˆ†æ•°", round(avg_sentiment, 2))

            out_df.to_csv("tweets_with_sentiment.csv", index=False, encoding="utf-8")
            st.download_button("ä¸‹è½½ç»“æœ CSV", data=open("tweets_with_sentiment.csv", "rb").read(), file_name="tweets_with_sentiment.csv")
        else:
            st.info("æœªæŠ“å–åˆ°æ¨æ–‡ã€‚")

with tab4:
    st.subheader("4ï¼šæ¨æ–‡æ•°é‡æŸ¥è¯¢æ¨¡å—")
    st.caption("æ³¨ï¼šæ­¤å¤„ä¸ºè¿‘ä¼¼ç»Ÿè®¡ï¼Œä¸¥æ ¼è®¡æ•°éœ€åˆ†é¡µç´¯ç§¯æˆ–å®˜æ–¹ counts ç«¯ç‚¹ã€‚")
    company = st.text_input("å…¬å¸è´¦å·ï¼ˆusernameï¼Œä¸å«@ï¼‰")
    preset = st.selectbox("æ—¶é—´åŒºé—´ï¼ˆé¢„è®¾ï¼‰", ["", "å½“å¤©", "è¿™å‘¨", "å½“æœˆ", "å½“å‰å­£åº¦", "å½“å‰åŠå¹´", "ä»Šå¹´"])
    recent = st.selectbox("æœ€è¿‘åŒºé—´", ["", "æœ€è¿‘ä¸€å¤©", "æœ€è¿‘ä¸€å‘¨", "æœ€è¿‘ä¸€æœˆ", "æœ€è¿‘ä¸€å­£åº¦", "æœ€è¿‘åŠå¹´", "æœ€è¿‘ä¸€å¹´"])
    run_count = st.button("æŸ¥è¯¢")
    if run_count and company.strip():
        if preset:
            mapping = {
                "å½“å¤©": today_range,
                "è¿™å‘¨": this_week_range,
                "å½“æœˆ": this_month_range,
                "å½“å‰å­£åº¦": this_quarter_range,
                "å½“å‰åŠå¹´": this_half_year_range,
                "ä»Šå¹´": this_year_range,
            }
            start, end = mapping[preset]()
        elif recent:
            mapping_days = {
                "æœ€è¿‘ä¸€å¤©": 1,
                "æœ€è¿‘ä¸€å‘¨": 7,
                "æœ€è¿‘ä¸€æœˆ": 30,
                "æœ€è¿‘ä¸€å­£åº¦": 90,
                "æœ€è¿‘åŠå¹´": 180,
                "æœ€è¿‘ä¸€å¹´": 365,
            }
            start, end = recent_days_range(mapping_days[recent])
        else:
            st.warning("è¯·é€‰æ‹©é¢„è®¾æˆ–æœ€è¿‘åŒºé—´")
            start = end = None
        if start and end:
            # æ ¹æ®æƒé™é€‰æ‹©ä¸åŒçš„APIè°ƒç”¨æ–¹æ³•
            if not st.session_state.api_settings.use_search_all:
                # Basic Planï¼šä½¿ç”¨get_user_tweetsç«¯ç‚¹è·å–ç”¨æˆ·æ¨æ–‡æ•°é‡
                try:
                    # å…ˆè·å–ç”¨æˆ·ID
                    user_info = api.get_user_by_username(company)
                    if not user_info:
                        st.error(f"æœªæ‰¾åˆ°è´¦å·: {company}")
                        st.stop()

                    user_id = user_info.get("id")
                    # è°ƒç”¨get_user_tweetsè·å–æ¨æ–‡æ•°é‡
                    resp = api.get_user_tweets(
                        user_id=user_id,
                        start_time=start,
                        end_time=end,
                        exclude="replies,retweets"
                    )
                    total = len(resp.get("data", []))
                    st.metric("æ¨æ–‡æ•°é‡", total)
                except Exception as e:
                    st.error(f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            else:
                # Academicæƒé™ï¼šä½¿ç”¨search_tweetsç«¯ç‚¹
                query = f"from:{company} -is:reply"
                resp = api.search_tweets(query=query, start_time=start, end_time=end, max_results=100)
                total = resp.get("meta", {}).get("result_count", 0)
                st.metric("è¿‘ä¼¼æ¨æ–‡æ•°é‡", total)

with tab5:
    st.subheader("5ï¼šæ‰¹é‡è´¦å·æŸ¥æ‰¾ä¸ç®¡ç†")
    excel_batch = st.file_uploader("ä¸Šä¼ ã€æ¨ç‰¹å…¬å¸æ ·æœ¬.xlsxã€", type=["xlsx"], key="batch_accounts_xlsx")
    run_batch_accounts = st.button("æ‰¹é‡æŸ¥æ‰¾")
    if run_batch_accounts and excel_batch:
        buf = io.BytesIO(excel_batch.read())
        df = pd.read_excel(buf, engine="openpyxl")
        temp_path = "ä¸´æ—¶_æ‰¹é‡è´¦å·.xlsx"
        df.to_excel(temp_path, index=False)
        companies = read_first_column_as_list(temp_path)
        results: list[AccountInfo] = []
        error_count = 0
        try:
            with st.spinner("æ‰¹é‡æŸ¥æ‰¾ä¸­..."):
                for i, comp in enumerate(companies):
                    try:
                        acc = search_account_for_company(api, comp)
                        if acc:
                            results.append(acc)
                    except requests.exceptions.HTTPError as e:
                        if "401" in str(e):
                            st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                            break
                        elif "429" in str(e):
                            st.warning(f"æŸ¥æ‰¾{comp}æ—¶é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                            time.sleep(10)  # æš‚åœ10ç§’å†ç»§ç»­
                            # é‡è¯•ä¸€æ¬¡å½“å‰å…¬å¸
                            try:
                                acc = search_account_for_company(api, comp)
                                if acc:
                                    results.append(acc)
                            except Exception:
                                error_count += 1
                        else:
                            error_count += 1
                            st.warning(f"æŸ¥æ‰¾{comp}æ—¶å‡ºé”™ï¼š{str(e)}")
                            continue
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()
        if error_count > 0:
            st.warning(f"å®Œæˆæ‰¹é‡æŸ¥æ‰¾ï¼Œä½†æœ‰ {error_count} ä¸ªå…¬å¸å¤„ç†å‡ºé”™ã€‚")
        if results:
            out_df = pd.DataFrame([r.__dict__ for r in results])
            out_df.to_csv("company_account_map.csv", index=False, encoding="utf-8")
            st.dataframe(out_df, use_container_width=True)
            st.download_button("ä¸‹è½½è´¦å·æ˜ å°„ CSV", data=open("company_account_map.csv", "rb").read(), file_name="company_account_map.csv")
        else:
            st.info("æœªæ‰¾åˆ°ä»»ä½•è´¦å·ã€‚")

with tab6:
    st.subheader("6ï¼šæ‰¹é‡æ¨æ–‡æ•°é‡æŸ¥è¯¢ï¼ˆåŸºäºã€æ¨ç‰¹å…¬å¸æ ·æœ¬_1570.xlsxã€ï¼‰")
    excel_counts = st.file_uploader("ä¸Šä¼  Excelï¼ˆéœ€åŒ…å«å…¬å¸åä¸æ—¥æœŸä¸¤åˆ—ï¼‰", type=["xlsx"], key="batch_counts_xlsx")
    run_batch_counts = st.button("å¼€å§‹ç»Ÿè®¡")
    if run_batch_counts and excel_counts:
        buf = io.BytesIO(excel_counts.read())
        df = pd.read_excel(buf, engine="openpyxl")
        company_col = df.columns[0]
        date_col = df.columns[1]
        rows = []
        error_count = 0
        try:
            with st.spinner("ç»Ÿè®¡ä¸­..."):
                for i, row in df.iterrows():
                    company = normalize_company_name(str(row[company_col]))
                    the_date = pd.to_datetime(row[date_col]).date()
                    try:
                        # æ ¹æ®æƒé™é€‰æ‹©ä¸åŒçš„APIè°ƒç”¨æ–¹æ³•
                        if not st.session_state.api_settings.use_search_all:
                            # Basic Planï¼šä½¿ç”¨get_user_tweetsç«¯ç‚¹
                            # å…ˆè·å–ç”¨æˆ·ID
                            user_info = api.get_user_by_username(company)
                            if not user_info:
                                st.warning(f"æœªæ‰¾åˆ°è´¦å·: {company}")
                                error_count += 1
                                continue

                            user_id = user_info.get("id")

                            # å½“å¤©ç»Ÿè®¡
                            day_start = f"{the_date.strftime('%Y-%m-%d')}T00:00:00Z"
                            day_end = f"{the_date.strftime('%Y-%m-%d')}T23:59:59Z"
                            resp_day = api.get_user_tweets(
                                user_id=user_id,
                                start_time=day_start,
                                end_time=day_end,
                                exclude="replies,retweets"
                            )
                            count_day = len(resp_day.get("data", []))

                            # Basic Planåªæ”¯æŒæœ€è¿‘7å¤©ï¼Œæ‰€ä»¥Â±180å¤©ç»Ÿè®¡æ”¹ä¸ºÂ±3å¤©
                            start_iso = f"{(the_date - timedelta(days=3)).strftime('%Y-%m-%d')}T00:00:00Z"
                            end_iso = f"{(the_date + timedelta(days=3)).strftime('%Y-%m-%d')}T23:59:59Z"
                            resp_week = api.get_user_tweets(
                                user_id=user_id,
                                start_time=start_iso,
                                end_time=end_iso,
                                exclude="replies,retweets"
                            )
                            count_week = len(resp_week.get("data", []))
                            rows.append({"å…¬å¸åç§°": company, "æ—¥æœŸ": str(the_date), "å½“å¤©æ¨æ–‡æ•°": count_day, "Â±3å¤©æ¨æ–‡æ•°": count_week})
                        else:
                            # Academicæƒé™ï¼šä½¿ç”¨search_tweetsç«¯ç‚¹
                            # Â±180 å¤©
                            start_iso = f"{(the_date - timedelta(days=180)).strftime('%Y-%m-%d')}T00:00:00Z"
                            end_iso = f"{(the_date + timedelta(days=180)).strftime('%Y-%m-%d')}T23:59:59Z"
                            query = f"from:{company} -is:reply"
                            resp_180 = api.search_tweets(query=query, start_time=start_iso, end_time=end_iso, max_results=100)
                            count_180 = resp_180.get("meta", {}).get("result_count", 0)
                            # å½“å¤©
                            day_start = f"{the_date.strftime('%Y-%m-%d')}T00:00:00Z"
                            day_end = f"{the_date.strftime('%Y-%m-%d')}T23:59:59Z"
                            resp_day = api.search_tweets(query=query, start_time=day_start, end_time=day_end, max_results=100)
                            count_day = resp_day.get("meta", {}).get("result_count", 0)
                            rows.append({"å…¬å¸åç§°": company, "æ—¥æœŸ": str(the_date), "å½“å¤©æ¨æ–‡æ•°": count_day, "Â±180å¤©æ¨æ–‡æ•°": count_180})
                    except requests.exceptions.HTTPError as e:
                        if "401" in str(e):
                            st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                            break
                        elif "429" in str(e):
                            st.warning(f"ç»Ÿè®¡{company}æ—¶é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                            time.sleep(15)  # æ‰¹é‡æ“ä½œä¸­æš‚åœæ›´ä¹…
                            error_count += 1
                        else:
                            error_count += 1
                            continue
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()
        if error_count > 0:
            st.warning(f"å®Œæˆæ‰¹é‡ç»Ÿè®¡ï¼Œä½†æœ‰ {error_count} æ¡è®°å½•å¤„ç†å‡ºé”™ã€‚")
        out_df = pd.DataFrame(rows)
        out_df.to_csv("counts_ç»“æœ.csv", index=False, encoding="utf-8")
        st.dataframe(out_df, use_container_width=True)
        st.download_button("ä¸‹è½½ç»Ÿè®¡ç»“æœ CSV", data=open("counts_ç»“æœ.csv", "rb").read(), file_name="counts_ç»“æœ.csv")

with tab7:
    st.subheader("7ï¼šæ‰¹é‡æ¨æ–‡å†…å®¹æŸ¥è¯¢ï¼ˆåŸºäºã€æ¨ç‰¹å…¬å¸æ ·æœ¬_è¯¦ç»†.xlsxã€ï¼‰")
    excel_contents = st.file_uploader("ä¸Šä¼  Excelï¼ˆéœ€åŒ…å«å…¬å¸åä¸æ—¥æœŸä¸¤åˆ—ï¼‰", type=["xlsx"], key="batch_contents_xlsx")
    window = st.number_input("çª—å£å¤©æ•°ï¼ˆÂ±windowï¼‰", value=180, min_value=1, max_value=365)
    run_batch_contents = st.button("å¼€å§‹æŠ“å–")
    if run_batch_contents and excel_contents:
        buf = io.BytesIO(excel_contents.read())
        df = pd.read_excel(buf, engine="openpyxl")
        company_col = df.columns[0]
        date_col = df.columns[1]
        all_rows = []
        error_count = 0
        try:
            with st.spinner("æŠ“å–ä¸­..."):
                for i, row in df.iterrows():
                    comp = normalize_company_name(str(row[company_col]))
                    the_date = pd.to_datetime(row[date_col]).date()
                    start_iso = f"{(the_date - timedelta(days=window)).strftime('%Y-%m-%d')}T00:00:00Z"
                    end_iso = f"{(the_date + timedelta(days=window)).strftime('%Y-%m-%d')}T23:59:59Z"
                    try:
                        # æ ¹æ®æƒé™é€‰æ‹©ä¸åŒçš„APIè°ƒç”¨æ–¹æ³•
                        if not st.session_state.api_settings.use_search_all:
                            # Basic Planï¼šä½¿ç”¨get_user_tweetsç«¯ç‚¹
                            # å…ˆè·å–ç”¨æˆ·ID
                            user_info = api.get_user_by_username(comp)
                            if not user_info:
                                st.warning(f"æœªæ‰¾åˆ°è´¦å·: {comp}")
                                error_count += 1
                                continue

                            user_id = user_info.get("id")

                            # è°ƒæ•´æ—¶é—´èŒƒå›´ä¸ºBasic Planæ”¯æŒçš„æœ€è¿‘7å¤©å†…
                            # è®¡ç®—å®é™…å¯æŸ¥è¯¢çš„æ—¶é—´èŒƒå›´
                            current_date = datetime.now().date()
                            min_date = current_date - timedelta(days=7)

                            # è°ƒæ•´æŸ¥è¯¢çš„å¼€å§‹æ—¶é—´ä¸æ—©äº7å¤©å‰
                            actual_start_date = max((the_date - timedelta(days=window)), min_date)
                            actual_end_date = min((the_date + timedelta(days=window)), current_date)

                            start_iso = f"{actual_start_date.strftime('%Y-%m-%d')}T00:00:00Z"
                            end_iso = f"{actual_end_date.strftime('%Y-%m-%d')}T23:59:59Z"

                            has_next = True
                            next_token = None
                            while has_next:
                                try:
                                    resp = api.get_user_tweets(
                                        user_id=user_id,
                                        start_time=start_iso,
                                        end_time=end_iso,
                                        exclude="replies,retweets",
                                        pagination_token=next_token,
                                        max_results=100
                                    )
                                    data = resp.get("data", [])
                                    for t in data:
                                        text = t.get("text", "").replace("\n", " ")
                                        all_rows.append({"å…¬å¸åç§°": comp, "æ¨æ–‡å†…å®¹": text, "å‘å¸ƒæ—¶é—´": t.get("created_at"), "æƒ…ç»ªåˆ†æ•°": sentiment_score(text)})

                                    next_token = resp.get("meta", {}).get("next_token")
                                    has_next = bool(next_token)
                                except requests.exceptions.HTTPError as e:
                                    if "401" in str(e):
                                        st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                                        has_next = False
                                        break
                                    elif "429" in str(e):
                                        st.warning(f"æŠ“å–{comp}çš„æ¨æ–‡æ—¶é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                                        time.sleep(20)
                                        continue
                                    else:
                                        st.warning(f"æŠ“å–{comp}çš„æ¨æ–‡æ—¶å‡ºé”™ï¼š{str(e)}")
                                        has_next = False
                        else:
                            # Academicæƒé™ï¼šä½¿ç”¨search_tweetsç«¯ç‚¹
                            query = f"from:{comp}"
                            has_next = True
                            next_token = None
                            while has_next:
                                try:
                                    resp = api.search_tweets(
                                        query=query, 
                                        start_time=start_iso, 
                                        end_time=end_iso, 
                                        expansions=["author_id"], 
                                        max_results=100,
                                        next_token=next_token
                                    )
                                    data = resp.get("data", [])
                                    for t in data:
                                        text = t.get("text", "").replace("\n", " ")
                                        all_rows.append({"å…¬å¸åç§°": comp, "æ¨æ–‡å†…å®¹": text, "å‘å¸ƒæ—¶é—´": t.get("created_at"), "æƒ…ç»ªåˆ†æ•°": sentiment_score(text)})

                                    next_token = resp.get("meta", {}).get("next_token")
                                    has_next = bool(next_token)
                                except requests.exceptions.HTTPError as e:
                                    if "401" in str(e):
                                        st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                                        has_next = False
                                        break
                                    elif "429" in str(e):
                                        st.warning(f"æŠ“å–{comp}çš„æ¨æ–‡æ—¶é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                                        time.sleep(20)  # æ‰¹é‡å†…å®¹æŠ“å–æš‚åœæ›´ä¹…
                                        continue  # é‡è¯•å½“å‰è¯·æ±‚
                                    else:
                                        st.warning(f"æŠ“å–{comp}çš„æ¨æ–‡æ—¶å‡ºé”™ï¼š{str(e)}")
                                        has_next = False
                    except Exception as e:
                        error_count += 1
                        st.warning(f"å¤„ç†{comp}æ—¶å‡ºé”™ï¼š{str(e)}")
                        continue
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()
        if error_count > 0:
            st.warning(f"å®Œæˆæ‰¹é‡æŠ“å–ï¼Œä½†æœ‰ {error_count} ä¸ªå…¬å¸å¤„ç†å‡ºé”™ã€‚")
        out_df = pd.DataFrame(all_rows)
        out_df.to_csv("contents_ç»“æœ.csv", index=False, encoding="utf-8")
        st.dataframe(out_df, use_container_width=True)
        st.download_button("ä¸‹è½½å†…å®¹ç»“æœ CSV", data=open("contents_ç»“æœ.csv", "rb").read(), file_name="contents_ç»“æœ.csv")

# å…¨å±€å˜é‡ç”¨äºè·Ÿè¸ªAPIé™åˆ¶
if 'api_limits' not in st.session_state:
    st.session_state.api_limits = {
        'monthly_tweets_count': 0,  # æ¯æœˆæ¨æ–‡è®¡æ•°å™¨
        'user_requests': {},  # æ¯ç”¨æˆ·è¯·æ±‚è·Ÿè¸ª {user_handle: [timestamp1, timestamp2, ...]}
        'app_requests': [],  # åº”ç”¨çº§è¯·æ±‚æ—¶é—´æˆ³åˆ—è¡¨
        'last_reset_date': datetime.now().date()  # ä¸Šæ¬¡é‡ç½®æ—¥æœŸ
    }

with tab8:
    st.subheader("8ï¼šæ‰¹é‡æŸ¥è¯¢æ¨æ–‡")
    st.caption("ä»…æ”¯æŒå½“å‰é…ç½®ï¼šBasic Planï¼ˆä»…æ”¯æŒæœ€è¿‘7å¤©ï¼‰")
    st.caption("APIé™åˆ¶ï¼šæ¯æœˆæœ€å¤š15Kæ¡æ¨æ–‡ï¼Œæ¯ç”¨æˆ·æ¯15åˆ†é’Ÿ5ä¸ªè¯·æ±‚ï¼Œæ¯åº”ç”¨æ¯15åˆ†é’Ÿ10ä¸ªè¯·æ±‚")

    # æ‰¹é‡å¯¼å…¥æ¨ç‰¹è´¦å·åŠŸèƒ½
    uploaded_file = st.file_uploader("ä¸Šä¼ è´¦å·æ–‡ä»¶ï¼ˆæ”¯æŒCSVå’ŒExcelæ ¼å¼ï¼Œä¸‰åˆ—ï¼šå…¬å¸åç§°,æ¨ç‰¹è´¦å·,æ¨ç‰¹IDï¼‰", type=["csv", "xlsx", "xls"])

    if uploaded_file is not None:
        # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒCSVå’ŒExcelï¼‰
        try:
            logger.info(f"å¼€å§‹å¤„ç†ä¸Šä¼ æ–‡ä»¶: {uploaded_file.name}")
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©ä¸åŒçš„è¯»å–æ–¹æ³•
            file_extension = uploaded_file.name.split('.')[-1].lower()
            logger.info(f"æ–‡ä»¶ç±»å‹: {file_extension}")
            if file_extension in ['xlsx', 'xls']:
                accounts_df = pd.read_excel(uploaded_file)
                logger.info("æˆåŠŸè¯»å–Excelæ–‡ä»¶")
            else:  # csv
                accounts_df = pd.read_csv(uploaded_file)
                logger.info("æˆåŠŸè¯»å–CSVæ–‡ä»¶")
            if len(accounts_df.columns) < 3:
                st.error("ä¸Šä¼ çš„æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ç¡®ä¿æ–‡ä»¶åŒ…å«è‡³å°‘ä¸‰åˆ—ï¼šå…¬å¸åç§°ã€æ¨ç‰¹è´¦å·å’Œæ¨ç‰¹ID")
            else:
                # æ˜¾ç¤ºä¸Šä¼ çš„æ•°æ®é¢„è§ˆ
                logger.info(f"å…±è¯»å– {len(accounts_df)} ä¸ªè´¦å·")
                st.write("ä¸Šä¼ çš„è´¦å·åˆ—è¡¨ï¼š")
                st.dataframe(accounts_df, use_container_width=True)

                # æ·»åŠ æ–‡ä»¶æ ¼å¼è¯´æ˜
                st.info("æ–‡ä»¶æ ¼å¼è¯´æ˜ï¼š")
                st.info("- ç¬¬1åˆ—ï¼šå…¬å¸åç§°ï¼ˆå¿…å¡«ï¼‰")
                st.info("- ç¬¬2åˆ—ï¼šæ¨ç‰¹è´¦å·ï¼ˆå¿…å¡«ï¼Œæ ¼å¼ï¼š@usernameï¼‰")
                st.info("- ç¬¬3åˆ—ï¼šæ¨ç‰¹IDï¼ˆå¯é€‰ï¼‰")
                st.info("- ç¬¬4åˆ—ï¼šå¼€å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼Œæ ¼å¼ï¼šYYYY/MM/DDï¼‰")
                st.info("- ç¬¬5åˆ—ï¼šç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼Œæ ¼å¼ï¼šYYYY/MM/DDï¼‰")

                # æ‰¹é‡æŸ¥è¯¢æŒ‰é’®
                if st.button("å¼€å§‹æ‰¹é‡æŸ¥è¯¢æ¨æ–‡"):
                    logger.info("å¼€å§‹æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢æ¨æ–‡ä»»åŠ¡")
                    # æ£€æŸ¥å¹¶é‡ç½®æœˆåº¦è®¡æ•°ï¼ˆå¦‚æœæ˜¯æ–°æœˆä»½ï¼‰
                    current_date = datetime.now().date()
                    if current_date.month != st.session_state.api_limits['last_reset_date'].month:
                        logger.info(f"è·¨æœˆé‡ç½®ï¼Œæ–°æœˆä»½ï¼š{current_date.month}ï¼Œé‡ç½®æœˆåº¦æ¨æ–‡è®¡æ•°")
                        st.session_state.api_limits['monthly_tweets_count'] = 0
                        st.session_state.api_limits['last_reset_date'] = current_date

                    # ç»“æœåˆ—è¡¨
                    results = []
                    total_accounts = len(accounts_df)
                    progress_bar = st.progress(0)
                    logger.info(f"æ‰¹é‡æŸ¥è¯¢ä»»åŠ¡å¼€å§‹ï¼Œæ€»è´¦å·æ•°: {total_accounts}")

                    with st.spinner("æ­£åœ¨æ‰¹é‡æŸ¥è¯¢æ¨æ–‡ä¸­..."):
                        # åˆ›å»ºè´¦å·ä¿¡æ¯åˆ—è¡¨
                        accounts_info = []
                        for _, row in accounts_df.iterrows():
                            # è§£æåŸºæœ¬ä¿¡æ¯
                            company = str(row.iloc[0]).strip() if pd.notna(row.iloc[0]) else ''
                            handle = str(row.iloc[1]).strip() if pd.notna(row.iloc[1]) else ''
                            user_id = str(row.iloc[2]).strip() if pd.notna(row.iloc[2]) else ''

                            # è§£ææ—¥æœŸä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                            start_date = str(row.iloc[3]).strip() if len(row) > 3 and pd.notna(row.iloc[3]) else ''
                            end_date = str(row.iloc[4]).strip() if len(row) > 4 and pd.notna(row.iloc[4]) else ''

                            # æ¸…ç†æ¨ç‰¹è´¦å·ï¼ˆç§»é™¤@ç¬¦å·ï¼‰
                            if handle.startswith('@'):
                                handle = handle[1:]

                            # å…¬å¸åç§°å’Œæ¨ç‰¹è´¦å·ä¸ºå¿…å¡«
                            if company and handle:
                                accounts_info.append({
                                    'company': company,
                                    'handle': handle,
                                    'user_id': user_id,
                                    'start_date': start_date,
                                    'end_date': end_date
                                })
                            else:
                                logger.warning(f"è·³è¿‡æ— æ•ˆè´¦å·è¡Œï¼šå…¬å¸åç§°={company}ï¼Œæ¨ç‰¹è´¦å·={handle}")

                        logger.info(f"æˆåŠŸè§£æ {len(accounts_info)} ä¸ªæœ‰æ•ˆè´¦å·ä¿¡æ¯")
                        total_accounts = len(accounts_info)

                        # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•ï¼ˆè¶…è¿‡15åˆ†é’Ÿï¼‰
                        current_time = time.time()
                        fifteen_minutes_ago = current_time - (15 * 60)

                        # æ¸…ç†åº”ç”¨çº§è¯·æ±‚è®°å½•
                        st.session_state.api_limits['app_requests'] = [
                            t for t in st.session_state.api_limits['app_requests'] 
                            if t > fifteen_minutes_ago
                        ]

                        # æ¸…ç†æ¯ç”¨æˆ·è¯·æ±‚è®°å½•
                        for user in list(st.session_state.api_limits['user_requests'].keys()):
                            st.session_state.api_limits['user_requests'][user] = [
                                t for t in st.session_state.api_limits['user_requests'][user] 
                                if t > fifteen_minutes_ago
                            ]

                        # æ‰¹é‡å¤„ç†è´¦å·
                        break_loop = False  # ç”¨äºæ ‡è®°æ˜¯å¦éœ€è¦ä¸­æ–­å¾ªç¯
                        for index, account_info in enumerate(accounts_info):
                            if break_loop:
                                break
                            try:
                                # æ›´æ–°è¿›åº¦
                                progress_bar.progress((index + 1) / total_accounts)

                                # è·å–è´¦å·ä¿¡æ¯
                                company_name = account_info['company']
                                twitter_handle = account_info['handle']
                                user_id = account_info['user_id']
                                start_date_str = account_info.get('start_date', '')
                                end_date_str = account_info.get('end_date', '')

                                # ç»„è£…æ—¶é—´æ®µ
                                def assemble_time_period(start_date, end_date):
                                    # é»˜è®¤æ—¶é—´æ®µ
                                    default_start = "2025-11-14T00:00:00Z"
                                    default_end = "2025-11-19T23:59:59Z"

                                    # å¤„ç†ç©ºå€¼æƒ…å†µ
                                    if not start_date and not end_date:
                                        return default_start, default_end

                                    # å¤„ç†å¼€å§‹æ—¥æœŸ
                                    if start_date:
                                        try:
                                            # æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼: YYYY/MM/DD å’Œ YYYY-MM-DD 00:00:00
                                            if '/' in start_date:
                                                # YYYY/MM/DDæ ¼å¼
                                                year, month, day = map(int, start_date.split('/'))
                                            else:
                                                # YYYY-MM-DD 00:00:00æ ¼å¼
                                                date_part = start_date.split()[0]  # è·å–æ—¥æœŸéƒ¨åˆ†
                                                year, month, day = map(int, date_part.split('-'))
                                            
                                            # ä¿ç•™åŸå§‹å¹´ä»½
                                            start_iso = f"{year}-{month:02d}-{day:02d}T00:00:00Z"
                                        except:
                                            # æ ¼å¼é”™è¯¯æ—¶ä½¿ç”¨é»˜è®¤å€¼
                                            logger.warning(f"å¼€å§‹æ—¥æœŸæ ¼å¼é”™è¯¯: {start_date}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                                            start_iso = default_start
                                    else:
                                        start_iso = default_start

                                    # å¤„ç†ç»“æŸæ—¥æœŸ
                                    if end_date:
                                        try:
                                            # æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼: YYYY/MM/DD å’Œ YYYY-MM-DD 00:00:00
                                            if '/' in end_date:
                                                # YYYY/MM/DDæ ¼å¼
                                                year, month, day = map(int, end_date.split('/'))
                                            else:
                                                # YYYY-MM-DD 00:00:00æ ¼å¼
                                                date_part = end_date.split()[0]  # è·å–æ—¥æœŸéƒ¨åˆ†
                                                year, month, day = map(int, date_part.split('-'))
                                            
                                            # æ ¹æ®éœ€æ±‚ï¼Œå¹´ä»½å›ºå®šä¸º2025æˆ–ä½¿ç”¨åŸå¹´ä»½
                                            end_iso = f"{year}-{month:02d}-{day:02d}T23:59:59Z"
                                        except:
                                            # æ ¼å¼é”™è¯¯æ—¶ä½¿ç”¨é»˜è®¤å€¼
                                            logger.warning(f"ç»“æŸæ—¥æœŸæ ¼å¼é”™è¯¯: {end_date}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                                            end_iso = default_end
                                    else:
                                        # å¦‚æœç»“æŸæ—¥æœŸä¸ºç©ºï¼Œåœ¨å¼€å§‹æ—¥æœŸåŸºç¡€ä¸Š+6å¤©
                                        if start_date:
                                            try:
                                                # è§£æå¼€å§‹æ—¥æœŸå¹¶åŠ ä¸Š6å¤©ï¼Œæ”¯æŒå¤šç§æ ¼å¼
                                                if '/' in start_date:
                                                    # YYYY/MM/DDæ ¼å¼
                                                    year, month, day = map(int, start_date.split('/'))
                                                else:
                                                    # YYYY-MM-DD 00:00:00æ ¼å¼
                                                    date_part = start_date.split()[0]  # è·å–æ—¥æœŸéƒ¨åˆ†
                                                    year, month, day = map(int, date_part.split('-'))
                                                
                                                # ä¿ç•™åŸå§‹å¹´ä»½
                                                from datetime import datetime, timedelta
                                                date_obj = datetime(year, month, day)
                                                end_date_obj = date_obj + timedelta(days=6)
                                                end_iso = f"{end_date_obj.year}-{end_date_obj.month:02d}-{end_date_obj.day:02d}T23:59:59Z"
                                            except:
                                                end_iso = default_end
                                        else:
                                            end_iso = default_end

                                    # éªŒè¯å¼€å§‹æ—¶é—´ä¸æ™šäºç»“æŸæ—¶é—´
                                    from datetime import datetime
                                    try:
                                        start_dt = datetime.fromisoformat(start_iso.replace('Z', '+00:00'))
                                        end_dt = datetime.fromisoformat(end_iso.replace('Z', '+00:00'))
                                        
                                        if start_dt > end_dt:
                                            logger.warning(f"å¼€å§‹æ—¶é—´({start_iso})æ™šäºç»“æŸæ—¶é—´({end_iso})ï¼Œå·²è‡ªåŠ¨è°ƒæ•´é¡ºåº")
                                            # äº¤æ¢å¼€å§‹æ—¶é—´å’Œç»“æŸæ—¶é—´
                                            start_iso, end_iso = end_iso, start_iso
                                    except Exception as e:
                                        logger.error(f"æ—¶é—´éªŒè¯å‡ºé”™: {str(e)}")
                                    
                                    return start_iso, end_iso

                                # è·å–æ—¶é—´æ®µ
                                start_time, end_time = assemble_time_period(start_date_str, end_date_str)
                                logger.info(f"æ—¶é—´æ®µ: {start_time} è‡³ {end_time}")

                                logger.info(f"å¤„ç†è´¦å·: {index+1}/{total_accounts} - {company_name} (@{twitter_handle}) ID:{user_id}")

                                # æ£€æŸ¥æœˆåº¦æ¨æ–‡é™åˆ¶
                                if st.session_state.api_limits['monthly_tweets_count'] >= 15000:
                                    logger.warning(f"å·²è¾¾åˆ°æ¯æœˆ15Kæ¡æ¨æ–‡çš„é™åˆ¶ï¼Œåœæ­¢æŸ¥è¯¢")
                                    st.warning(f"å·²è¾¾åˆ°æ¯æœˆ15Kæ¡æ¨æ–‡çš„é™åˆ¶ï¼Œæ— æ³•ç»§ç»­æŸ¥è¯¢ã€‚")
                                    break

                                # ä½¿ç”¨user_idæˆ–twitter_handleä½œä¸ºè¯·æ±‚è·Ÿè¸ªæ ‡è¯†
                                request_key = user_id if user_id else twitter_handle

                                # æ£€æŸ¥æ¯ç”¨æˆ·è¯·æ±‚é™åˆ¶ï¼ˆæ¯15åˆ†é’Ÿ5ä¸ªè¯·æ±‚ï¼‰
                                if request_key not in st.session_state.api_limits['user_requests']:
                                    st.session_state.api_limits['user_requests'][request_key] = []

                                user_requests_count = len(st.session_state.api_limits['user_requests'][request_key])
                                if user_requests_count >= 5:
                                    logger.warning(f"ç”¨æˆ·è¯·æ±‚é™åˆ¶: {company_name} (@{twitter_handle}) å·²è¾¾åˆ°æ¯15åˆ†é’Ÿ5ä¸ªè¯·æ±‚çš„é™åˆ¶ï¼Œè·³è¿‡")
                                    st.warning(f"{company_name} (@{twitter_handle}) å·²è¾¾åˆ°æ¯15åˆ†é’Ÿ5ä¸ªè¯·æ±‚çš„é™åˆ¶ï¼Œè·³è¿‡è¯¥è´¦å·ã€‚")
                                    continue

                                # æ£€æŸ¥åº”ç”¨çº§è¯·æ±‚é™åˆ¶ï¼ˆæ¯15åˆ†é’Ÿ10ä¸ªè¯·æ±‚ï¼‰
                                app_requests_count = len(st.session_state.api_limits['app_requests'])
                                if app_requests_count >= 10:
                                    st.warning(f"å·²è¾¾åˆ°æ¯åº”ç”¨æ¯15åˆ†é’Ÿ10ä¸ªè¯·æ±‚çš„é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚")
                                    break

                                st.write(f"æ­£åœ¨æŸ¥è¯¢ {company_name} (@{twitter_handle}) çš„æ¨æ–‡...")

                                # ä¼˜å…ˆä½¿ç”¨å·²æœ‰çš„user_idï¼Œå¦‚æœæ²¡æœ‰åˆ™é€šè¿‡usernameè·å–
                                if not user_id and twitter_handle:
                                    logger.info(f"é€šè¿‡ç”¨æˆ·åè·å–ç”¨æˆ·ID: @{twitter_handle}")
                                    try:
                                        user = api.get_user_by_username(twitter_handle)
                                        if not user:
                                            logger.error(f"ç”¨æˆ·ä¸å­˜åœ¨: @{twitter_handle}")
                                            st.error(f"æ— æ³•æ‰¾åˆ°ç”¨æˆ·: @{twitter_handle}")
                                            continue
                                        user_id = user['id']
                                        logger.info(f"è·å–åˆ°ç”¨æˆ·ID: {user_id} (@{twitter_handle})")
                                    except Exception as e:
                                        logger.error(f"è·å–ç”¨æˆ·IDå¤±è´¥: @{twitter_handle} - {str(e)}")
                                        st.error(f"è·å–ç”¨æˆ·IDå¤±è´¥: @{twitter_handle} - {str(e)}")
                                        continue
                                elif not user_id and not twitter_handle:
                                    logger.warning(f"ç¼ºå°‘å¿…è¦çš„è´¦å·ä¿¡æ¯: {company_name}")
                                    st.warning(f"è´¦å· {company_name} ç¼ºå°‘æ¨ç‰¹è´¦å·å’Œæ¨ç‰¹IDï¼Œè·³è¿‡")
                                    continue

                                # è°ƒç”¨get_user_tweetsæ–¹æ³•ï¼Œä½¿ç”¨ç»„è£…çš„æ—¶é—´æ®µå‚æ•°
                                logger.info(f"è°ƒç”¨APIè·å–æ¨æ–‡: @{twitter_handle} (ID: {user_id})ï¼Œæ—¶é—´æ®µ: {start_time} è‡³ {end_time}")
                                response = api.get_user_tweets(
                                    user_id=user_id,
                                    start_time=start_time,  # ä½¿ç”¨ç»„è£…çš„å¼€å§‹æ—¶é—´
                                    end_time=end_time,      # ä½¿ç”¨ç»„è£…çš„ç»“æŸæ—¶é—´
                                    max_results=100,        # ä½¿ç”¨æœ€å¤§é™åˆ¶
                                    exclude="replies,retweets",  # æ’é™¤å›å¤å’Œè½¬å‘
                                    tweet_fields="id,text,created_at,public_metrics,author_id"
                                )
                                logger.info(f"APIè°ƒç”¨å®Œæˆ: @{twitter_handle}ï¼Œå“åº”çŠ¶æ€: æˆåŠŸ")

                                # æ›´æ–°è¯·æ±‚è®¡æ•°
                                current_time = time.time()
                                st.session_state.api_limits['app_requests'].append(current_time)
                                st.session_state.api_limits['user_requests'][request_key].append(current_time)

                                # å¤„ç†è¿”å›çš„æ¨æ–‡
                                tweets = response.get("data", [])
                                logger.info(f"@{twitter_handle} è¿”å›æ¨æ–‡æ•°é‡: {len(tweets)}")

                                # è´¦å·æ‹‰å–ä¸ºç©ºæ—¶çš„å¤„ç†
                                if not tweets:
                                    logger.info(f"@{twitter_handle} åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰å‘å¸ƒä»»ä½•æ¨æ–‡")
                                    st.info(f"@{twitter_handle} åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰å‘å¸ƒä»»ä½•æ¨æ–‡")
                                else:
                                    # æ£€æŸ¥æ˜¯å¦ä¼šè¶…å‡ºæœˆåº¦é™åˆ¶
                                    tweets_to_process = min(len(tweets), 15000 - st.session_state.api_limits['monthly_tweets_count'])
                                    if tweets_to_process < len(tweets):
                                        st.info(f"å¤„ç† {tweets_to_process} æ¡æ¨æ–‡ï¼ˆè¾¾åˆ°æœˆåº¦é™åˆ¶ï¼‰")
                                    else:
                                        st.write(f"æ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡")

                                    # æ›´æ–°æœˆåº¦æ¨æ–‡è®¡æ•°
                                    st.session_state.api_limits['monthly_tweets_count'] += tweets_to_process
                                    logger.info(f"æ›´æ–°æœˆåº¦æ¨æ–‡è®¡æ•°: å½“å‰ç´¯è®¡ {st.session_state.api_limits['monthly_tweets_count']}/15000")

                                    # ä¸ºæ¯æ¡æ¨æ–‡è®¡ç®—æƒ…ç»ªåˆ†æ•°å¹¶æ·»åŠ åˆ°ç»“æœï¼ˆä½†ä¸è¶…è¿‡æœˆåº¦é™åˆ¶ï¼‰
                                    for t in tweets[:tweets_to_process]:
                                        # è®¡ç®—æƒ…ç»ªåˆ†æ•°
                                        sentiment = sentiment_score(t.get("text", ""))

                                        # è®¡ç®—æƒ…ç»ªçŠ¶æ€
                                        if sentiment > 0.05:
                                            sentiment_status = "æ­£é¢"
                                        elif sentiment < -0.05:
                                            sentiment_status = "è´Ÿé¢"
                                        else:
                                            sentiment_status = "ä¸­æ€§"

                                        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦çš„å­—æ®µ
                                        public_metrics = t.get("public_metrics", {})
                                        results.append({
                                            "åºå·": len(results) + 1,
                                            "å…¬å¸åç§°": company_name,
                                            "æ¨ç‰¹è´¦å·": f"@{twitter_handle}",
                                            "æ¨ç‰¹ID": user_id,
                                            "å¼€å§‹æ—¥æœŸ": start_date_str,
                                            "ç»“æŸæ—¥æœŸ": end_date_str,
                                            "æ¨ç‰¹å†…å®¹": t.get("text", ""),
                                            "å‘å¸ƒæ—¶é—´": t.get("created_at", ""),
                                            "æƒ…ç»ªåˆ†æ•°": sentiment,
                                            "æƒ…ç»ªçŠ¶æ€": sentiment_status,
                                            "è½¬æ¨æ•°": public_metrics.get("retweet_count", 0),
                                            "å›å¤æ•°": public_metrics.get("reply_count", 0),
                                            "ç‚¹èµæ•°": public_metrics.get("like_count", 0),
                                            "å¼•ç”¨æ•°": public_metrics.get("quote_count", 0)
                                        })

                            except Exception as e:
                                error_str = str(e)
                                # å¤„ç†400é”™è¯¯ï¼ˆé”™è¯¯è¯·æ±‚ï¼‰- ç›´æ¥é€€å‡ºå¾ªç¯
                                if "400" in error_str:
                                    logger.error(f"ä¸¥é‡é”™è¯¯: {company_name} (@{twitter_handle}) ID:{user_id} - è¯·æ±‚é”™è¯¯(400): {error_str}ï¼Œç»ˆæ­¢æ‰¹é‡æŸ¥è¯¢")
                                    st.error(f"è¯·æ±‚é”™è¯¯ (400)ï¼šå‚æ•°æ— æ•ˆæˆ–è¯·æ±‚æ ¼å¼é”™è¯¯ã€‚ç»ˆæ­¢æ‰¹é‡æŸ¥è¯¢ã€‚")
                                    # å°†é”™è¯¯æ ‡è®°ä¸ºéœ€è¦ä¸­æ–­å¾ªç¯
                                    break_loop = True
                                    break
                                # å¤„ç†401é”™è¯¯ï¼ˆè®¤è¯å¤±è´¥ï¼‰- ç›´æ¥é€€å‡ºå¾ªç¯
                                elif "401" in error_str:
                                    logger.error(f"ä¸¥é‡é”™è¯¯: {company_name} (@{twitter_handle}) ID:{user_id} - è®¤è¯å¤±è´¥(401): {error_str}ï¼Œç»ˆæ­¢æ‰¹é‡æŸ¥è¯¢")
                                    st.error(f"è®¤è¯å¤±è´¥ (401)ï¼šè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®ã€‚ç»ˆæ­¢æ‰¹é‡æŸ¥è¯¢ã€‚")
                                    # å°†é”™è¯¯æ ‡è®°ä¸ºéœ€è¦ä¸­æ–­å¾ªç¯
                                    break_loop = True
                                    break
                                # å¤„ç†429é”™è¯¯ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰- ç›´æ¥é€€å‡ºå¾ªç¯
                                elif "429" in error_str:
                                    logger.error(f"ä¸¥é‡é”™è¯¯: {company_name} (@{twitter_handle}) ID:{user_id} - é€Ÿç‡é™åˆ¶(429): {error_str}ï¼Œç»ˆæ­¢æ‰¹é‡æŸ¥è¯¢")
                                    st.error(f"è¯·æ±‚è¿‡äºé¢‘ç¹ (429)ï¼šå·²è¾¾åˆ°APIé€Ÿç‡é™åˆ¶ã€‚è¯·ç¨åå†è¯•ã€‚ç»ˆæ­¢æ‰¹é‡æŸ¥è¯¢ã€‚")
                                    # å°†é”™è¯¯æ ‡è®°ä¸ºéœ€è¦ä¸­æ–­å¾ªç¯
                                    break_loop = True
                                    break
                                # å…¶ä»–é”™è¯¯ï¼ˆè´¦å·ä¸å­˜åœ¨ã€ç½‘ç»œé—®é¢˜ç­‰ï¼‰- åªè·³è¿‡å½“å‰è´¦å·ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                                else:
                                    logger.error(f"é”™è¯¯: {company_name} (@{twitter_handle}) ID:{user_id} - {error_str}ï¼Œè·³è¿‡è¯¥è´¦å·")
                                    st.warning(f"å¤„ç† {company_name} (@{twitter_handle}) æ—¶å‡ºé”™ï¼š{error_str}")
                                    st.info(f"è·³è¿‡è¯¥è´¦å·ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")
                                    continue

                    # æ˜¾ç¤ºç»“æœ
                    logger.info(f"æ‰¹é‡æŸ¥è¯¢ä»»åŠ¡å®Œæˆï¼Œæ€»å…±å¤„ç† {len(results)} æ¡æ¨æ–‡")
                    if results:
                        # åˆ›å»ºåŒ…å«æ‰€æœ‰éœ€è¦å­—æ®µçš„DataFrame
                        results_df = pd.DataFrame(results, columns=[
                            "åºå·", "å…¬å¸åç§°", "æ¨ç‰¹è´¦å·", "æ¨ç‰¹ID", 
                            "å¼€å§‹æ—¥æœŸ", "ç»“æŸæ—¥æœŸ", "æ¨ç‰¹å†…å®¹", 
                            "å‘å¸ƒæ—¶é—´", "æƒ…ç»ªåˆ†æ•°", "æƒ…ç»ªçŠ¶æ€",
                            "è½¬æ¨æ•°", "å›å¤æ•°", "ç‚¹èµæ•°", "å¼•ç”¨æ•°"
                        ])
                        results_df.to_csv("æ‰¹é‡æŸ¥è¯¢æ¨æ–‡ç»“æœ.csv", index=False, encoding="utf-8")
                        logger.info("ç»“æœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶")
                        st.success(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼å…±è·å– {len(results)} æ¡æ¨æ–‡")

                        # æ˜¾ç¤ºå½“å‰APIä½¿ç”¨æƒ…å†µ
                        st.info(f"å½“å‰APIä½¿ç”¨æƒ…å†µï¼šæœ¬æœˆå·²å¤„ç† {st.session_state.api_limits['monthly_tweets_count']}/15000 æ¡æ¨æ–‡")
                        st.dataframe(results_df, use_container_width=True)

                        # æ—¥å¿—é…ç½®è¯´æ˜
                        with st.expander("ğŸ“ æ—¥å¿—é…ç½®è¯´æ˜", expanded=False):
                            st.markdown("""
                            **æ—¥å¿—æ–‡ä»¶ä¿¡æ¯ï¼š**
                            - **æ–‡ä»¶è·¯å¾„ï¼š** ä¸åº”ç”¨ç¨‹åºåœ¨åŒä¸€ç›®å½•ä¸‹
                            - **æ–‡ä»¶å‘½åæ ¼å¼ï¼š** `twitter_crawler_YYYYMMDD_HHMMSS.log`
                            - **æ—¥å¿—çº§åˆ«ï¼š** INFOåŠä»¥ä¸Šï¼ˆåŒ…æ‹¬INFOã€WARNINGã€ERRORï¼‰
                            - **æ—¥å¿—å†…å®¹ï¼š** è®°å½•APIè°ƒç”¨ã€é”™è¯¯ä¿¡æ¯ã€æ–‡ä»¶å¤„ç†ç­‰å…³é”®æ“ä½œ
                            - **è¯´æ˜ï¼š** æ¯ä¸ªæ–°å¯åŠ¨çš„åº”ç”¨å®ä¾‹ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„æ—¥å¿—æ–‡ä»¶ï¼Œæ–‡ä»¶ååŒ…å«å¯åŠ¨æ—¶é—´æˆ³
                            """)

                        # å¯¼å‡ºåŠŸèƒ½ - åŒæ—¶æ”¯æŒCSVå’ŒExcelæ ¼å¼

                        # å¯¼å‡ºCSV
                        csv_data = results_df.to_csv(index=False, encoding="utf-8-sig")
                        logger.info("å‡†å¤‡CSVæ ¼å¼ä¸‹è½½æ•°æ®")
                        st.download_button(
                            "ä¸‹è½½æŸ¥è¯¢ç»“æœ (CSV)",
                            data=csv_data,
                            file_name="æ‰¹é‡æŸ¥è¯¢æ¨æ–‡ç»“æœ.csv",
                            mime="text/csv"
                        )

                        # å¯¼å‡ºExcel
                        output = io.BytesIO()
                        with pd.ExcelWriter(output, engine='openpyxl') as writer:
                            results_df.to_excel(writer, index=False, sheet_name='æ¨æ–‡æŸ¥è¯¢ç»“æœ')
                        excel_data = output.getvalue()
                        logger.info("å‡†å¤‡Excelæ ¼å¼ä¸‹è½½æ•°æ®")
                        st.download_button(
                            "ä¸‹è½½æŸ¥è¯¢ç»“æœ (Excel)",
                            data=excel_data,
                            file_name="æ‰¹é‡æŸ¥è¯¢æ¨æ–‡ç»“æœ.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    else:
                        st.info("æœªæ‰¾åˆ°ä»»ä½•æ¨æ–‡")
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
            st.error(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")


