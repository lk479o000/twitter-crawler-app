import io
import time
from datetime import datetime, timedelta

import pandas as pd
import requests
import streamlit as st

from twitter_crawler.data_prep import prepare_from_excel, normalize_company_name, read_first_column_as_list
from twitter_crawler.twitter_api import TwitterAPI
from twitter_crawler.accounts import search_account_for_company
from twitter_crawler.sentiment import sentiment_score
from twitter_crawler.storage import AccountInfo, save_accounts_csv, load_accounts_csv
from twitter_crawler.utils_date import (
    today_range, this_week_range, this_month_range, this_quarter_range,
    this_half_year_range, this_year_range, recent_days_range
)
from twitter_crawler.config import get_settings, Settings


st.set_page_config(page_title="æ¨ç‰¹æŠ“å–ä¸åˆ†æå·¥å…·", page_icon="ğŸ§©", layout="wide")
st.title("ğŸ§© æ¨ç‰¹æŠ“å–ä¸åˆ†æå·¥å…·ï¼ˆStreamlitï¼‰")
st.caption("æ”¯æŒï¼šæ•°æ®å‡†å¤‡ã€è´¦å·æŸ¥æ‰¾ã€æ¨æ–‡æŠ“å–ä¸æƒ…ç»ªã€æ•°é‡æŸ¥è¯¢ã€æ‰¹é‡ä»»åŠ¡")

# ä¾§è¾¹æ ï¼šAPI ä¸é€Ÿç‡é™åˆ¶è®¾ç½®ï¼ˆæ˜¾ç¤ºé»˜è®¤å€¼å¹¶å…è®¸ä¿®æ”¹ï¼‰
st.sidebar.header("API ä¸é€Ÿç‡é™åˆ¶è®¾ç½®")
default_settings = get_settings()
if "api_settings" not in st.session_state:
    st.session_state.api_settings = default_settings

with st.sidebar.expander("æŸ¥çœ‹/ä¿®æ”¹è®¾ç½®", expanded=True):
    token = st.text_input("Bearer Tokenï¼ˆç•™ç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰", value="", type="password")
    use_all = st.checkbox("ä½¿ç”¨å…¨é‡å†å² /tweets/search/allï¼ˆéœ€ Academicï¼‰", value=st.session_state.api_settings.use_search_all)
    rpm = st.number_input("æ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼ˆRequests Per Minuteï¼‰", min_value=1, max_value=300, value=st.session_state.api_settings.requests_per_minute, step=1)
    max_retries = st.number_input("æœ€å¤§é‡è¯•æ¬¡æ•°", min_value=0, max_value=20, value=st.session_state.api_settings.rate_limit_max_retries, step=1)
    base_delay = st.number_input("åŸºç¡€é€€é¿ç§’æ•°", min_value=0.0, max_value=120.0, value=float(st.session_state.api_settings.rate_limit_base_delay_seconds), step=0.5)
    max_delay = st.number_input("æœ€å¤§é€€é¿ç§’æ•°", min_value=0.0, max_value=600.0, value=float(st.session_state.api_settings.rate_limit_max_delay_seconds), step=1.0)
    apply_cfg = st.button("åº”ç”¨è®¾ç½®")
    if apply_cfg:
        st.session_state.api_settings = Settings(
            twitter_bearer_token=token or default_settings.twitter_bearer_token,
            use_search_all=use_all,
            rate_limit_max_retries=int(max_retries),
            rate_limit_base_delay_seconds=float(base_delay),
            rate_limit_max_delay_seconds=float(max_delay),
            requests_per_minute=int(rpm),
        )
        st.success("è®¾ç½®å·²åº”ç”¨")

# æ„é€  API å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨å½“å‰è®¾ç½®ï¼‰
api = TwitterAPI(
    bearer_token=st.session_state.api_settings.twitter_bearer_token,
    settings=st.session_state.api_settings
)

tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
    "1ï¼šæ•°æ®å‡†å¤‡ä¸æ¸…æ´—",
    "2ï¼šæ¨ç‰¹è´¦å·æŸ¥æ‰¾ä¸ç®¡ç†",
    "3ï¼šæ¨æ–‡æŠ“å–ä¸æƒ…ç»ªåˆ†æ",
    "4ï¼šæ¨æ–‡æ•°é‡æŸ¥è¯¢",
    "5ï¼šæ‰¹é‡è´¦å·æŸ¥æ‰¾ä¸ç®¡ç†",
    "6ï¼šæ‰¹é‡æ¨æ–‡æ•°é‡æŸ¥è¯¢",
    "7ï¼šæ‰¹é‡æ¨æ–‡å†…å®¹æŸ¥è¯¢"
])

with tab1:
    st.subheader("1ï¼šæ•°æ®å‡†å¤‡ä¸æ¸…æ´—")
    excel_file = st.file_uploader("ä¸Šä¼ ã€æ¨ç‰¹å…¬å¸æ ·æœ¬.xlsxã€", type=["xlsx"])
    col_a, col_b = st.columns(2)
    with col_a:
        run_prep = st.button("å¼€å§‹æ¸…æ´—å¹¶ç”Ÿæˆæ˜ å°„")
    if run_prep and excel_file:
        with st.spinner("å¤„ç†ä¸­..."):
            buf = io.BytesIO(excel_file.read())
            # å°†ä¸Šä¼ æ–‡ä»¶å†™åˆ°ä¸´æ—¶ DataFrame å¤„ç†
            df = pd.read_excel(buf, engine="openpyxl")
            temp_path = "ä¸´æ—¶_æ¨ç‰¹å…¬å¸æ ·æœ¬.xlsx"
            df.to_excel(temp_path, index=False)
            unique_names, mapping = prepare_from_excel(temp_path, "normalized_companies.csv", "name_mapping.csv")
        st.success(f"å®Œæˆï¼æ ‡å‡†åŒ–å…¬å¸æ•°ï¼š{len(unique_names)}")
        st.download_button("ä¸‹è½½ æ ‡å‡†åŒ–å…¬å¸åˆ—è¡¨ CSV", data=open("normalized_companies.csv","rb").read(), file_name="normalized_companies.csv")
        st.download_button("ä¸‹è½½ åç§°æ˜ å°„ CSV", data=open("name_mapping.csv","rb").read(), file_name="name_mapping.csv")

with tab2:
    st.subheader("2ï¼šæ¨ç‰¹è´¦å·æŸ¥æ‰¾ä¸ç®¡ç†")
    st.caption("ä¼˜å…ˆ verifiedï¼›å¤šç»“æœå¯åœ¨ CLI ä¸­ç¡®è®¤ã€‚æ­¤å¤„æä¾›åŸºç¡€å¿«é€ŸåŒ¹é…ã€‚")
    names_input = st.text_area("è¾“å…¥å…¬å¸åç§°ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    col1, col2 = st.columns(2)
    with col1:
        run_find = st.button("æŸ¥æ‰¾è´¦å·")
    with col2:
        uploaded_map = st.file_uploader("å¯¼å…¥ç°æœ‰è´¦å·æ˜ å°„ CSVï¼ˆå¯é€‰ï¼‰", type=["csv"])
    results_df = None
    if run_find and names_input.strip():
        names = [normalize_company_name(x) for x in names_input.splitlines() if x.strip()]
        results: list[AccountInfo] = []
        try:
            with st.spinner("æ­£åœ¨æŸ¥æ‰¾..."):
                for comp in names:
                    try:
                        acc = search_account_for_company(api, comp)
                        if acc:
                            results.append(acc)
                    except requests.exceptions.HTTPError as e:
                        if "401" in str(e):
                            st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                            break
                        elif "429" in str(e):
                            st.warning(f"æŸ¥æ‰¾{comp}æ—¶é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                            time.sleep(5)  # æš‚åœ5ç§’å†ç»§ç»­
                        else:
                            st.warning(f"æŸ¥æ‰¾{comp}æ—¶å‡ºé”™ï¼š{str(e)}")
                            continue
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()
        if results:
            results_df = pd.DataFrame([a.__dict__ for a in results])
            st.dataframe(results_df, use_container_width=True)
            save_accounts_csv(results, "company_account_map.csv")
            st.download_button("ä¸‹è½½è´¦å·æ˜ å°„ CSV", data=open("company_account_map.csv","rb").read(), file_name="company_account_map.csv")
        else:
            st.info("æœªæ‰¾åˆ°ä»»ä½•è´¦å·ï¼Œè¯·å°è¯•ä¸åŒåç§°ã€‚")
    if uploaded_map is not None:
        df_map = pd.read_csv(uploaded_map)
        st.dataframe(df_map, use_container_width=True)

with tab3:
    st.subheader("3ï¼šæ¨æ–‡æŠ“å–ä¸æƒ…ç»ªåˆ†æ")
    by = st.selectbox("æŸ¥è¯¢æ–¹å¼", ["æŒ‰è´¦å·", "æŒ‰å…³é”®å­—"])
    value = st.text_input("è´¦å·ï¼ˆä¸å«@ï¼‰æˆ–å…³é”®å­—")
    col1, col2, col3 = st.columns(3)
    with col1:
        start_date = st.date_input("å¼€å§‹æ—¥æœŸ", datetime(2006,1,1).date())
    with col2:
        end_date = st.date_input("ç»“æŸæ—¥æœŸ", datetime(2022,12,31).date())
    with col3:
        include_retweets = st.checkbox("åŒ…å«è½¬æ¨", value=True)
    run_fetch = st.button("æŠ“å–å¹¶åˆ†æ")
    if run_fetch and value.strip():
        query = f"from:{value}" if by == "æŒ‰è´¦å·" else value
        if not include_retweets:
            query += " -is:retweet"
        start_iso = f"{start_date.strftime('%Y-%m-%d')}T00:00:00Z"
        end_iso = f"{end_date.strftime('%Y-%m-%d')}T23:59:59Z"
        all_rows = []
        try:
            with st.spinner("æŠ“å–æ¨æ–‡..."):
                while True:
                    try:
                        resp = api.search_tweets(
                            query=query,
                            start_time=start_iso,
                            end_time=end_iso,
                            expansions=["author_id"],
                            max_results=100,
                        )
                        data = resp.get("data", [])
                        for t in data:
                            text = t.get("text","").replace("\n", " ")
                            all_rows.append({
                                "å…¬å¸åç§°": value if by == "æŒ‰è´¦å·" else "",
                                "æ¨æ–‡å†…å®¹": text,
                                "å‘å¸ƒæ—¶é—´": t.get("created_at"),
                                "æƒ…ç»ªåˆ†æ•°": sentiment_score(text),
                            })
                        next_token = resp.get("meta", {}).get("next_token")
                        if not next_token:
                            break
                    except requests.exceptions.HTTPError as e:
                        if "401" in str(e):
                            st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                            break
                        elif "429" in str(e):
                            st.warning("é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                            time.sleep(10)  # æš‚åœ10ç§’å†ç»§ç»­
                        else:
                            st.error(f"æŠ“å–æ¨æ–‡æ—¶å‡ºé”™ï¼š{str(e)}")
                            break
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()
        if all_rows:
            out_df = pd.DataFrame(all_rows)
            st.dataframe(out_df, use_container_width=True)
            out_df.to_csv("tweets_with_sentiment.csv", index=False, encoding="utf-8")
            st.download_button("ä¸‹è½½ç»“æœ CSV", data=open("tweets_with_sentiment.csv","rb").read(), file_name="tweets_with_sentiment.csv")
        else:
            st.info("æœªæŠ“å–åˆ°æ¨æ–‡ã€‚")

with tab4:
    st.subheader("4ï¼šæ¨æ–‡æ•°é‡æŸ¥è¯¢æ¨¡å—")
    st.caption("æ³¨ï¼šæ­¤å¤„ä¸ºè¿‘ä¼¼ç»Ÿè®¡ï¼Œä¸¥æ ¼è®¡æ•°éœ€åˆ†é¡µç´¯ç§¯æˆ–å®˜æ–¹ counts ç«¯ç‚¹ã€‚")
    company = st.text_input("å…¬å¸è´¦å·ï¼ˆusernameï¼Œä¸å«@ï¼‰")
    preset = st.selectbox("æ—¶é—´åŒºé—´ï¼ˆé¢„è®¾ï¼‰", ["", "å½“å¤©", "è¿™å‘¨", "å½“æœˆ", "å½“å‰å­£åº¦", "å½“å‰åŠå¹´", "ä»Šå¹´"])
    recent = st.selectbox("æœ€è¿‘åŒºé—´", ["", "æœ€è¿‘ä¸€å¤©", "æœ€è¿‘ä¸€å‘¨", "æœ€è¿‘ä¸€æœˆ", "æœ€è¿‘ä¸€å­£åº¦", "æœ€è¿‘åŠå¹´", "æœ€è¿‘ä¸€å¹´"])
    run_count = st.button("æŸ¥è¯¢")
    if run_count and company.strip():
        if preset:
            mapping = {
                "å½“å¤©": today_range,
                "è¿™å‘¨": this_week_range,
                "å½“æœˆ": this_month_range,
                "å½“å‰å­£åº¦": this_quarter_range,
                "å½“å‰åŠå¹´": this_half_year_range,
                "ä»Šå¹´": this_year_range,
            }
            start, end = mapping[preset]()
        elif recent:
            mapping_days = {
                "æœ€è¿‘ä¸€å¤©": 1,
                "æœ€è¿‘ä¸€å‘¨": 7,
                "æœ€è¿‘ä¸€æœˆ": 30,
                "æœ€è¿‘ä¸€å­£åº¦": 90,
                "æœ€è¿‘åŠå¹´": 180,
                "æœ€è¿‘ä¸€å¹´": 365,
            }
            start, end = recent_days_range(mapping_days[recent])
        else:
            st.warning("è¯·é€‰æ‹©é¢„è®¾æˆ–æœ€è¿‘åŒºé—´")
            start = end = None
        if start and end:
            query = f"from:{company} -is:reply"
            resp = api.search_tweets(query=query, start_time=start, end_time=end, max_results=100)
            total = resp.get("meta", {}).get("result_count", 0)
            st.metric("è¿‘ä¼¼æ¨æ–‡æ•°é‡", total)

with tab5:
    st.subheader("5ï¼šæ‰¹é‡è´¦å·æŸ¥æ‰¾ä¸ç®¡ç†")
    excel_batch = st.file_uploader("ä¸Šä¼ ã€æ¨ç‰¹å…¬å¸æ ·æœ¬.xlsxã€", type=["xlsx"], key="batch_accounts_xlsx")
    run_batch_accounts = st.button("æ‰¹é‡æŸ¥æ‰¾")
    if run_batch_accounts and excel_batch:
        buf = io.BytesIO(excel_batch.read())
        df = pd.read_excel(buf, engine="openpyxl")
        temp_path = "ä¸´æ—¶_æ‰¹é‡è´¦å·.xlsx"
        df.to_excel(temp_path, index=False)
        companies = read_first_column_as_list(temp_path)
        results: list[AccountInfo] = []
        error_count = 0
        try:
            with st.spinner("æ‰¹é‡æŸ¥æ‰¾ä¸­..."):
                for i, comp in enumerate(companies):
                    try:
                        acc = search_account_for_company(api, comp)
                        if acc:
                            results.append(acc)
                    except requests.exceptions.HTTPError as e:
                        if "401" in str(e):
                            st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                            break
                        elif "429" in str(e):
                            st.warning(f"æŸ¥æ‰¾{comp}æ—¶é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                            time.sleep(10)  # æš‚åœ10ç§’å†ç»§ç»­
                            # é‡è¯•ä¸€æ¬¡å½“å‰å…¬å¸
                            try:
                                acc = search_account_for_company(api, comp)
                                if acc:
                                    results.append(acc)
                            except Exception:
                                error_count += 1
                        else:
                            error_count += 1
                            st.warning(f"æŸ¥æ‰¾{comp}æ—¶å‡ºé”™ï¼š{str(e)}")
                            continue
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()
        if error_count > 0:
            st.warning(f"å®Œæˆæ‰¹é‡æŸ¥æ‰¾ï¼Œä½†æœ‰ {error_count} ä¸ªå…¬å¸å¤„ç†å‡ºé”™ã€‚")
        if results:
            out_df = pd.DataFrame([r.__dict__ for r in results])
            out_df.to_csv("company_account_map.csv", index=False, encoding="utf-8")
            st.dataframe(out_df, use_container_width=True)
            st.download_button("ä¸‹è½½è´¦å·æ˜ å°„ CSV", data=open("company_account_map.csv","rb").read(), file_name="company_account_map.csv")
        else:
            st.info("æœªæ‰¾åˆ°ä»»ä½•è´¦å·ã€‚")

with tab6:
    st.subheader("6ï¼šæ‰¹é‡æ¨æ–‡æ•°é‡æŸ¥è¯¢ï¼ˆåŸºäºã€æ¨ç‰¹å…¬å¸æ ·æœ¬_1570.xlsxã€ï¼‰")
    excel_counts = st.file_uploader("ä¸Šä¼  Excelï¼ˆéœ€åŒ…å«å…¬å¸åä¸æ—¥æœŸä¸¤åˆ—ï¼‰", type=["xlsx"], key="batch_counts_xlsx")
    run_batch_counts = st.button("å¼€å§‹ç»Ÿè®¡")
    if run_batch_counts and excel_counts:
        buf = io.BytesIO(excel_counts.read())
        df = pd.read_excel(buf, engine="openpyxl")
        company_col = df.columns[0]
        date_col = df.columns[1]
        rows = []
        error_count = 0
        try:
            with st.spinner("ç»Ÿè®¡ä¸­..."):
                for i, row in df.iterrows():
                    company = normalize_company_name(str(row[company_col]))
                    the_date = pd.to_datetime(row[date_col]).date()
                    try:
                        # Â±180 å¤©
                        start_iso = f"{(the_date - timedelta(days=180)).strftime('%Y-%m-%d')}T00:00:00Z"
                        end_iso = f"{(the_date + timedelta(days=180)).strftime('%Y-%m-%d')}T23:59:59Z"
                        query = f"from:{company} -is:reply"
                        resp_180 = api.search_tweets(query=query, start_time=start_iso, end_time=end_iso, max_results=100)
                        count_180 = resp_180.get("meta", {}).get("result_count", 0)
                        # å½“å¤©
                        day_start = f"{the_date.strftime('%Y-%m-%d')}T00:00:00Z"
                        day_end = f"{the_date.strftime('%Y-%m-%d')}T23:59:59Z"
                        resp_day = api.search_tweets(query=query, start_time=day_start, end_time=day_end, max_results=100)
                        count_day = resp_day.get("meta", {}).get("result_count", 0)
                        rows.append({"å…¬å¸åç§°": company, "æ—¥æœŸ": str(the_date), "å½“å¤©æ¨æ–‡æ•°": count_day, "Â±180å¤©æ¨æ–‡æ•°": count_180})
                    except requests.exceptions.HTTPError as e:
                        if "401" in str(e):
                            st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                            break
                        elif "429" in str(e):
                            st.warning(f"ç»Ÿè®¡{company}æ—¶é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                            time.sleep(15)  # æ‰¹é‡æ“ä½œä¸­æš‚åœæ›´ä¹…
                            error_count += 1
                        else:
                            error_count += 1
                            continue
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()
        if error_count > 0:
            st.warning(f"å®Œæˆæ‰¹é‡ç»Ÿè®¡ï¼Œä½†æœ‰ {error_count} æ¡è®°å½•å¤„ç†å‡ºé”™ã€‚")
        out_df = pd.DataFrame(rows)
        out_df.to_csv("counts_ç»“æœ.csv", index=False, encoding="utf-8")
        st.dataframe(out_df, use_container_width=True)
        st.download_button("ä¸‹è½½ç»Ÿè®¡ç»“æœ CSV", data=open("counts_ç»“æœ.csv","rb").read(), file_name="counts_ç»“æœ.csv")

with tab7:
    st.subheader("7ï¼šæ‰¹é‡æ¨æ–‡å†…å®¹æŸ¥è¯¢ï¼ˆåŸºäºã€æ¨ç‰¹å…¬å¸æ ·æœ¬_è¯¦ç»†.xlsxã€ï¼‰")
    excel_contents = st.file_uploader("ä¸Šä¼  Excelï¼ˆéœ€åŒ…å«å…¬å¸åä¸æ—¥æœŸä¸¤åˆ—ï¼‰", type=["xlsx"], key="batch_contents_xlsx")
    window = st.number_input("çª—å£å¤©æ•°ï¼ˆÂ±windowï¼‰", value=180, min_value=1, max_value=365)
    run_batch_contents = st.button("å¼€å§‹æŠ“å–")
    if run_batch_contents and excel_contents:
        buf = io.BytesIO(excel_contents.read())
        df = pd.read_excel(buf, engine="openpyxl")
        company_col = df.columns[0]
        date_col = df.columns[1]
        all_rows = []
        error_count = 0
        try:
            with st.spinner("æŠ“å–ä¸­..."):
                for i, row in df.iterrows():
                    comp = normalize_company_name(str(row[company_col]))
                    the_date = pd.to_datetime(row[date_col]).date()
                    start_iso = f"{(the_date - timedelta(days=window)).strftime('%Y-%m-%d')}T00:00:00Z"
                    end_iso = f"{(the_date + timedelta(days=window)).strftime('%Y-%m-%d')}T23:59:59Z"
                    query = f"from:{comp}"
                    try:
                        has_next = True
                        next_token = None
                        while has_next:
                            try:
                                resp = api.search_tweets(
                                    query=query, 
                                    start_time=start_iso, 
                                    end_time=end_iso, 
                                    expansions=["author_id"], 
                                    max_results=100,
                                    next_token=next_token
                                )
                                data = resp.get("data", [])
                                for t in data:
                                    text = t.get("text","").replace("\n", " ")
                                    all_rows.append({"å…¬å¸åç§°": comp, "æ¨æ–‡å†…å®¹": text, "å‘å¸ƒæ—¶é—´": t.get("created_at"), "æƒ…ç»ªåˆ†æ•°": sentiment_score(text)})
                                next_token = resp.get("meta", {}).get("next_token")
                                has_next = bool(next_token)
                            except requests.exceptions.HTTPError as e:
                                if "401" in str(e):
                                    st.error(f"è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Bearer Tokenæ˜¯å¦æœ‰æ•ˆã€‚")
                                    has_next = False
                                    break
                                elif "429" in str(e):
                                    st.warning(f"æŠ“å–{comp}çš„æ¨æ–‡æ—¶é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œå°†æš‚åœä¸€æ®µæ—¶é—´åç»§ç»­...")
                                    time.sleep(20)  # æ‰¹é‡å†…å®¹æŠ“å–æš‚åœæ›´ä¹…
                                    continue  # é‡è¯•å½“å‰è¯·æ±‚
                                else:
                                    st.warning(f"æŠ“å–{comp}çš„æ¨æ–‡æ—¶å‡ºé”™ï¼š{str(e)}")
                                    has_next = False
                    except Exception as e:
                        error_count += 1
                        st.warning(f"å¤„ç†{comp}æ—¶å‡ºé”™ï¼š{str(e)}")
                        continue
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.stop()
        if error_count > 0:
            st.warning(f"å®Œæˆæ‰¹é‡æŠ“å–ï¼Œä½†æœ‰ {error_count} ä¸ªå…¬å¸å¤„ç†å‡ºé”™ã€‚")
        out_df = pd.DataFrame(all_rows)
        out_df.to_csv("contents_ç»“æœ.csv", index=False, encoding="utf-8")
        st.dataframe(out_df, use_container_width=True)
        st.download_button("ä¸‹è½½å†…å®¹ç»“æœ CSV", data=open("contents_ç»“æœ.csv","rb").read(), file_name="contents_ç»“æœ.csv")


