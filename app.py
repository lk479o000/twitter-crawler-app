import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import time
import random
import requests
from bs4 import BeautifulSoup
import json

# è®¾ç½®é¡µé¢
st.set_page_config(
    page_title="å…¬å¸Twitteræ•°æ®æŠ“å–",
    page_icon="ğŸ¦",
    layout="wide"
)


class TwitterScraper:
    def __init__(self):
        self.company_handles = {
            'AAPL': 'Apple',
            'TSLA': 'Tesla',
            'MSFT': 'Microsoft',
            'GOOGL': 'Google',
            'AMZN': 'Amazon',
            'META': 'Meta',
            'NFLX': 'Netflix',
            'NVDA': 'NVIDIA',
            'JPM': 'jpmorgan',
            'JNJ': 'JNJNews',
            'V': 'Visa',
            'WMT': 'Walmart',
            'DIS': 'Disney',
            'BA': 'Boeing',
            'INTC': 'Intel',
            'CSCO': 'Cisco',
            'IBM': 'IBM',
            'GS': 'GoldmanSachs'
        }

    def get_company_twitter_handles(self):
        """è·å–å…¬å¸Twitterè´¦å·æ˜ å°„"""
        return self.company_handles

    def scrape_twitter_alternative(self, username, start_date, end_date, max_tweets=20):
        """ä½¿ç”¨æ›¿ä»£æ–¹æ³•æŠ“å–çœŸå®Twitteræ•°æ®"""
        try:
            st.info(f"æ­£åœ¨æŠ“å– @{username} çš„çœŸå®æ•°æ®...")

            # æ–¹æ³•1: ä½¿ç”¨ Nitter é•œåƒï¼ˆTwitterçš„å…¬å¼€æ›¿ä»£ï¼‰
            tweets = self.scrape_via_nitter(username, start_date, end_date, max_tweets)

            if tweets:
                return tweets

            # æ–¹æ³•2: ä½¿ç”¨å…¬å¼€APIç«¯ç‚¹
            st.warning(f"Nitter æŠ“å–å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
            tweets = self.scrape_via_public_api(username, max_tweets)

            if tweets:
                return tweets

            # æ–¹æ³•3: é™çº§åˆ°æ¨¡æ‹Ÿæ•°æ®
            st.error(f"æ— æ³•è·å– @{username} çš„çœŸå®æ•°æ®ï¼Œä½¿ç”¨é«˜è´¨é‡æ¨¡æ‹Ÿæ•°æ®")
            return self.generate_high_quality_mock_data(username, start_date, end_date, max_tweets)

        except Exception as e:
            st.error(f"æŠ“å– @{username} æ—¶å‡ºé”™: {str(e)}")
            return self.generate_high_quality_mock_data(username, start_date, end_date, max_tweets)

    def scrape_via_nitter(self, username, start_date, end_date, max_tweets):
        """é€šè¿‡ Nitter é•œåƒæŠ“å–æ•°æ®"""
        try:
            # ä½¿ç”¨ Nitter å®ä¾‹ï¼ˆTwitterçš„å…¬å¼€é•œåƒï¼‰
            nitter_instances = [
                "https://nitter.net",
                "https://nitter.privacydev.net",
                "https://nitter.poast.org"
            ]

            tweets = []

            for instance in nitter_instances:
                try:
                    url = f"{instance}/{username}"
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }

                    response = requests.get(url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')

                        # è§£ææ¨æ–‡ï¼ˆNitter çš„HTMLç»“æ„ï¼‰
                        tweet_elements = soup.find_all('div', class_='timeline-item')

                        for i, tweet in enumerate(tweet_elements[:max_tweets]):
                            try:
                                content_elem = tweet.find('div', class_='tweet-content')
                                if content_elem:
                                    content = content_elem.get_text(strip=True)

                                    # è·å–äº’åŠ¨æ•°æ®
                                    stats = tweet.find('div', class_='tweet-stats')
                                    like_count = 0
                                    retweet_count = 0

                                    if stats:
                                        like_elem = stats.find('span', class_='tweet-stat')
                                        if like_elem:
                                            like_text = like_elem.get_text(strip=True)
                                            like_count = self.extract_number(like_text)

                                    tweet_data = {
                                        'tweet_id': f'nitter_{username}_{i}',
                                        'username': username,
                                        'content': content,
                                        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                        'timestamp': datetime.now(),
                                        'like_count': like_count,
                                        'retweet_count': retweet_count,
                                        'reply_count': 0,
                                        'quote_count': 0,
                                        'view_count': 0,
                                        'url': f"{instance}/{username}/status/{i}",
                                        'has_media': False,
                                        'language': 'en',
                                        'source': 'nitter'
                                    }
                                    tweets.append(tweet_data)
                            except Exception as e:
                                continue

                        if tweets:
                            st.success(f"é€šè¿‡ Nitter è·å–åˆ° {len(tweets)} æ¡çœŸå®æ¨æ–‡")
                            return tweets

                except Exception as e:
                    continue

            return []

        except Exception as e:
            return []

    def scrape_via_public_api(self, username, max_tweets):
        """é€šè¿‡å…¬å¼€APIç«¯ç‚¹å°è¯•æŠ“å–"""
        try:
            # ä½¿ç”¨ Twitter çš„å…¬å¼€åµŒå…¥API
            embed_url = f"https://publish.twitter.com/oembed?url=https://twitter.com/{username}"

            response = requests.get(embed_url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                # è¿™é‡Œå¯ä»¥è§£æè¿”å›çš„åµŒå…¥æ•°æ®
                # ä½†ç”±äºé™åˆ¶ï¼Œé€šå¸¸åªèƒ½è·å–æœ‰é™ä¿¡æ¯
                pass

            return []
        except:
            return []

    def extract_number(self, text):
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—"""
        try:
            # å¤„ç† "1.2K", "5M" ç­‰æ ¼å¼
            if 'K' in text:
                return int(float(text.replace('K', '').strip()) * 1000)
            elif 'M' in text:
                return int(float(text.replace('M', '').strip()) * 1000000)
            else:
                return int(''.join(filter(str.isdigit, text)))
        except:
            return 0

    def generate_high_quality_mock_data(self, username, start_date, end_date, max_tweets):
        """ç”Ÿæˆé«˜è´¨é‡çš„æ¨¡æ‹Ÿæ•°æ®ï¼ˆå½“çœŸå®æŠ“å–å¤±è´¥æ—¶ï¼‰"""
        tweets = []
        base_date = datetime.strptime(start_date, "%Y-%m-%d")
        end_date_obj = datetime.strptime(end_date, "%Y-%m-%d")
        days_range = max(1, (end_date_obj - base_date).days)

        # åŸºäºçœŸå®å…¬å¸æ•°æ®çš„æ¨æ–‡æ¨¡æ¿
        real_tweet_templates = {
            'Apple': [
                "Introducing the new iPhone with revolutionary features",
                "Our commitment to privacy and security continues",
                "Apple Watch Series now available with health monitoring",
                "iOS update brings new productivity features",
                "Sustainability report: our environmental progress"
            ],
            'Tesla': [
                "New software update improves autopilot performance",
                "Gigafactory production reaches new milestones",
                "Tesla Solar Roof now available in new regions",
                "Charging network expansion continues globally",
                "Quarterly vehicle delivery numbers announced"
            ],
            'Microsoft': [
                "Windows 11 update with new AI features",
                "Azure cloud services expand to new regions",
                "LinkedIn reaches 1 billion members milestone",
                "Xbox Game Pass new titles announced",
                "Microsoft 365 Copilot now generally available"
            ]
        }

        templates = real_tweet_templates.get(username, [
            "Company earnings report shows strong growth",
            "New product launch announcement",
            "Sustainability and ESG initiatives update",
            "Partnership with industry leaders",
            "Corporate responsibility report published"
        ])

        num_tweets = min(max_tweets, 15)

        for i in range(num_tweets):
            tweet_date = base_date + timedelta(days=random.randint(0, days_range))

            # åŸºäºçœŸå®æ•°æ®çš„äº’åŠ¨èŒƒå›´
            if username in ['Apple', 'Tesla', 'Microsoft']:
                likes = random.randint(5000, 50000)
                retweets = random.randint(500, 5000)
                views = random.randint(100000, 1000000)
            else:
                likes = random.randint(1000, 20000)
                retweets = random.randint(100, 2000)
                views = random.randint(50000, 500000)

            tweet_data = {
                'tweet_id': f'realistic_{username}_{i}_{int(tweet_date.timestamp())}',
                'username': username,
                'content': f"{random.choice(templates)} - {tweet_date.strftime('%b %d')}",
                'date': tweet_date.strftime('%Y-%m-%d %H:%M:%S'),
                'timestamp': tweet_date,
                'like_count': likes,
                'retweet_count': retweets,
                'reply_count': random.randint(50, 500),
                'quote_count': random.randint(10, 200),
                'view_count': views,
                'url': f"https://twitter.com/{username}/status/real_{i}",
                'has_media': random.choice([True, False]),
                'language': 'en',
                'source': 'simulated_real_data'
            }
            tweets.append(tweet_data)

        tweets.sort(key=lambda x: x['timestamp'])
        return tweets

    def get_company_info(self, ticker):
        """è·å–å…¬å¸åŸºæœ¬ä¿¡æ¯"""
        company_names = {
            'AAPL': 'Apple Inc.',
            'TSLA': 'Tesla Inc.',
            'MSFT': 'Microsoft Corporation',
            'GOOGL': 'Alphabet Inc. (Google)',
            'AMZN': 'Amazon.com Inc.',
            'META': 'Meta Platforms Inc.',
            'NFLX': 'Netflix Inc.',
            'NVDA': 'NVIDIA Corporation',
            'JPM': 'JPMorgan Chase & Co.',
            'JNJ': 'Johnson & Johnson',
            'V': 'Visa Inc.',
            'WMT': 'Walmart Inc.',
            'DIS': 'The Walt Disney Company',
            'BA': 'The Boeing Company',
            'INTC': 'Intel Corporation',
            'CSCO': 'Cisco Systems, Inc.',
            'IBM': 'International Business Machines Corporation',
            'GS': 'The Goldman Sachs Group, Inc.'
        }

        return {
            'ticker': ticker,
            'company_name': company_names.get(ticker, f"{ticker} Corporation"),
            'source': 'Company Database'
        }


def main():
    st.title("ğŸ¦ ç¾å›½ä¸Šå¸‚å…¬å¸Twitteræ•°æ®æŠ“å–å·¥å…·")
    st.markdown("---")

    # åˆå§‹åŒ–æŠ“å–å™¨
    scraper = TwitterScraper()

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ æŠ“å–å‚æ•°é…ç½®")

        # å…¬å¸é€‰æ‹©
        st.subheader("é€‰æ‹©å…¬å¸")
        company_handles = scraper.get_company_twitter_handles()

        selected_companies = st.multiselect(
            "é€‰æ‹©è¦æŠ“å–çš„å…¬å¸:",
            options=list(company_handles.keys()),
            format_func=lambda x: f"{x} - {company_handles[x]}",
            default=['AAPL', 'TSLA', 'MSFT']
        )

        # æ—¶é—´èŒƒå›´é€‰æ‹©
        st.markdown("---")
        st.subheader("æ—¶é—´èŒƒå›´")
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input(
                "å¼€å§‹æ—¥æœŸ:",
                value=datetime.now() - timedelta(days=30),
                max_value=datetime.now()
            )
        with col2:
            end_date = st.date_input(
                "ç»“æŸæ—¥æœŸ:",
                value=datetime.now(),
                max_value=datetime.now()
            )

        # éªŒè¯æ—¥æœŸèŒƒå›´
        if start_date > end_date:
            st.error("âŒ å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸï¼")
            return

        # å…¶ä»–å‚æ•°
        st.markdown("---")
        st.subheader("æŠ“å–è®¾ç½®")
        max_tweets = st.slider(
            "æ¯å®¶å…¬å¸æœ€å¤§æ¨æ–‡æ•°é‡:",
            min_value=5,
            max_value=50,
            value=15,
            step=5
        )

        st.markdown("---")
        st.info("""
        **æ•°æ®æ¥æºè¯´æ˜:**
        - ä¼˜å…ˆå°è¯•çœŸå®Twitteræ•°æ®æŠ“å–
        - ä½¿ç”¨Nitteré•œåƒä½œä¸ºæ›¿ä»£æ–¹æ¡ˆ
        - å¦‚çœŸå®æŠ“å–å¤±è´¥ï¼Œä½¿ç”¨é«˜è´¨é‡æ¨¡æ‹Ÿæ•°æ®
        - æ‰€æœ‰æ•°æ®åŸºäºçœŸå®å…¬å¸æ¨æ–‡æ¨¡å¼
        """)

    # ä¸»å†…å®¹åŒºåŸŸ
    col1, col2 = st.columns([2, 1])

    with col1:
        st.header("ğŸ“Š æŠ“å–æ§åˆ¶")

        # æ˜¾ç¤ºé…ç½®æ‘˜è¦
        st.subheader("å½“å‰é…ç½®")
        summary_col1, summary_col2, summary_col3 = st.columns(3)
        with summary_col1:
            st.metric("é€‰æ‹©å…¬å¸æ•°", len(selected_companies))
        with summary_col2:
            st.metric("æ—¶é—´èŒƒå›´", f"{(end_date - start_date).days} å¤©")
        with summary_col3:
            st.metric("æœ€å¤§æ¨æ–‡æ•°", max_tweets)

        # å¼€å§‹æŠ“å–æŒ‰é’®
        st.markdown("---")
        if st.button("ğŸš€ å¼€å§‹æŠ“å–æ•°æ®", type="primary", use_container_width=True):
            if not selected_companies:
                st.error("âŒ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªå…¬å¸ï¼")
                return

            all_tweets = []
            company_info_list = []

            # æ‰§è¡Œæ•°æ®æŠ“å–
            with st.spinner("æ­£åœ¨æŠ“å–æ•°æ®ï¼Œè¯·ç¨å€™..."):

                # è·å–å…¬å¸ä¿¡æ¯
                for ticker in selected_companies:
                    with st.expander(f"{ticker} å…¬å¸ä¿¡æ¯", expanded=False):
                        company_info = scraper.get_company_info(ticker)
                        company_info_list.append(company_info)
                        st.write(f"**å…¬å¸åç§°:** {company_info['company_name']}")
                        st.write(f"**è‚¡ç¥¨ä»£ç :** {company_info['ticker']}")
                        st.write(f"**Twitterè´¦å·:** @{company_handles.get(ticker, 'N/A')}")

                # æŠ“å–Twitteræ•°æ®
                for ticker in selected_companies:
                    username = company_handles.get(ticker)
                    if username:
                        with st.expander(f"æŠ“å– {ticker} (@{username}) çš„æ¨æ–‡", expanded=False):
                            tweets = scraper.scrape_twitter_alternative(
                                username=username,
                                start_date=start_date.strftime("%Y-%m-%d"),
                                end_date=end_date.strftime("%Y-%m-%d"),
                                max_tweets=max_tweets
                            )

                            for tweet in tweets:
                                tweet['company_ticker'] = ticker
                                tweet['company_name'] = company_handles.get(ticker, ticker)
                                all_tweets.append(tweet)

                            # æ˜¾ç¤ºæ•°æ®æ¥æº
                            if tweets and 'source' in tweets[0]:
                                source = tweets[0]['source']
                                if source == 'nitter':
                                    st.success(f"âœ… é€šè¿‡NitteræŠ“å–åˆ° {len(tweets)} æ¡çœŸå®æ¨æ–‡")
                                elif source == 'simulated_real_data':
                                    st.warning(f"âš ï¸ ä½¿ç”¨é«˜è´¨é‡æ¨¡æ‹Ÿæ•°æ® ({len(tweets)} æ¡)")
                                else:
                                    st.success(f"âœ… æˆåŠŸæŠ“å– {len(tweets)} æ¡æ¨æ–‡")

                    # å…¬å¸é—´å»¶è¿Ÿ
                    time.sleep(1)

            # å¤„ç†ç»“æœ
            if all_tweets:
                results_df = pd.DataFrame(all_tweets)

                # æ˜¾ç¤ºç»“æœç»Ÿè®¡
                st.success(f"ğŸ‰ æŠ“å–å®Œæˆï¼å…±è·å– {len(results_df)} æ¡æ¨æ–‡")

                # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                st.subheader("ğŸ“‹ æ•°æ®é¢„è§ˆ")

                display_columns = ['company_ticker', 'company_name', 'username', 'date',
                                   'content', 'like_count', 'retweet_count']
                available_columns = [col for col in display_columns if col in results_df.columns]

                st.dataframe(results_df[available_columns].head(10), use_container_width=True)

                # æ˜¾ç¤ºæ•°æ®æ¥æºç»Ÿè®¡
                st.subheader("ğŸ“Š æ•°æ®æ¥æºç»Ÿè®¡")
                if 'source' in results_df.columns:
                    source_counts = results_df['source'].value_counts()
                    for source, count in source_counts.items():
                        if source == 'nitter':
                            st.info(f"ğŸ”— NitterçœŸå®æ•°æ®: {count} æ¡")
                        elif source == 'simulated_real_data':
                            st.warning(f"ğŸ“Š æ¨¡æ‹Ÿæ•°æ®: {count} æ¡")
                        else:
                            st.success(f"âœ… {source}: {count} æ¡")

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                st.subheader("ğŸ“ˆ äº’åŠ¨ç»Ÿè®¡")
                stat_col1, stat_col2, stat_col3, stat_col4 = st.columns(4)
                with stat_col1:
                    st.metric("æ€»æ¨æ–‡æ•°", len(results_df))
                with stat_col2:
                    st.metric("æ¶‰åŠå…¬å¸æ•°", results_df['company_ticker'].nunique())
                with stat_col3:
                    avg_likes = results_df['like_count'].mean()
                    st.metric("å¹³å‡ç‚¹èµæ•°", f"{avg_likes:.0f}")
                with stat_col4:
                    avg_retweets = results_df['retweet_count'].mean()
                    st.metric("å¹³å‡è½¬æ¨æ•°", f"{avg_retweets:.0f}")

                # å¯¼å‡ºé€‰é¡¹
                st.subheader("ğŸ’¾ å¯¼å‡ºæ•°æ®")
                export_col1, export_col2, export_col3 = st.columns(3)

                with export_col1:
                    csv = results_df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½CSV",
                        data=csv,
                        file_name=f"twitter_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )

                # ä¿å­˜åˆ°session state
                st.session_state.results_df = results_df

            else:
                st.error("âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·è°ƒæ•´å‚æ•°é‡è¯•ã€‚")

    with col2:
        st.header("ğŸ“ˆ å®æ—¶çŠ¶æ€")

        if 'results_df' in st.session_state:
            st.success("âœ… ä¸Šæ¬¡æŠ“å–å®Œæˆ")
            results_df = st.session_state.results_df

            st.subheader("æ•°æ®æ¦‚è§ˆ")
            st.write(f"**æ€»æ•°æ®é‡:** {len(results_df)} æ¡æ¨æ–‡")
            st.write(f"**æ—¶é—´èŒƒå›´:** {results_df['date'].min().split()[0]} è‡³ {results_df['date'].max().split()[0]}")
            st.write(f"**æ¶‰åŠå…¬å¸:** {', '.join(results_df['company_ticker'].unique())}")

        else:
            st.info("â³ ç­‰å¾…å¼€å§‹æŠ“å–...")
            st.write("**æŠ“å–ç­–ç•¥:**")
            st.write("â€¢ ä¼˜å…ˆçœŸå®Twitteræ•°æ®")
            st.write("â€¢ Nitteré•œåƒä½œä¸ºå¤‡é€‰")
            st.write("â€¢ é«˜è´¨é‡æ¨¡æ‹Ÿæ•°æ®å…œåº•")

        if st.button("ğŸ”„ æ¸…é™¤ç¼“å­˜", use_container_width=True):
            if 'results_df' in st.session_state:
                del st.session_state.results_df
            st.rerun()


if __name__ == "__main__":
    main()